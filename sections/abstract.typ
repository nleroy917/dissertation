Chromatin accessibility profiling through scATAC-seq has emerged as a powerful tool for understanding gene regulation and cellular heterogeneity, yet existing analytical methods fail to leverage the nearly 10,000 datasets in the public repositories and remain computationally intensive for routine use. While existing tools adequately handle basic single-cell chromatin accessibility analysis, they are not architected to leverage modern deep learning approaches that could unlock insights across biological contexts and enable integration into advanced analytical workflows at scale. Recent advances in machine learning suggest a path forward. Transfer learning, particularly with large pre-trained models, has shown promise in scRNA-seq applications by enabling improved cell type clustering, identification, and dataset integration across studies. However, the application of transfer learning to scATAC-seq remains underexplored, despite similar structural and analytical challenges shared between the modalities.

This dissertation presents a comprehensive framework for scATAC-seq analysis that addresses these limitations through transformer-based transfer learning. The central innovation is to conceptualize genomic regions as discrete linguistic tokens, enabling direct adaptation of natural language processing techniques to epigenomic data. This approach facilitates the creation of open, generalizable foundation models that improve computational efficiency while enabling biological discovery across diverse datasets.

We first introduce the gtars toolkit, which provides efficient Rust-based utilities for creating consensus genomic interval vocabularies and tokenizing datasets into shared representations. These tools address critical infrastructure gaps in systematic vocabulary creation and efficient dataset mapping. Building on this foundation, we present scEmbed, a Word2Vec-inspired model that demonstrates the feasibility of tokenized chromatin accessibility modeling through fixed region embeddings that encode shared regulatory context. While scEmbed enables rapid clustering and cross-dataset cell-type annotation, its static embeddings reveal limitations in capturing cell-state-specific regulatory dynamics.

To address these constraints, we develop Atacformer, a transformer-based foundation model pre-trained on 1.2 million cells from 30 tissues. Through self-attention mechanisms, Atacformer generates contextualized representations that capture cell-level chromatin accessibility patterns and contextual dependencies within regulatory profiles. Unlike existing foundation models that use continuous representations and require large parameter counts, Atacformer maintains discrete token-level embeddings, achieving comparable performance with dramatically fewer parameters while preserving interpretability. We further introduce CRAFT, a dual-encoder architecture pairing Atacformer with Geneformer to enable cross-modal alignment between scATAC-seq and scRNA-seq data.

Across multiple benchmarks, these approaches demonstrate strong performance in zero-shot clustering, annotation, and batch correction while revealing biologically meaningful regulatory relationships at the single-region level, including the discovery of weak promoter elements. Together, these contributions establish a scalable, transferable framework that bridges isolated experiments with unified biological insights, democratizing deep learning approaches for chromatin accessibility analysis and laying the groundwork for standardized analysis of chromatin accessibility data across diverse biological contexts.