#import "@local/dissertation:0.0.1": *;

#figure(
  image("/fig/gtokenizers/uniwig.svg"),
  caption: [Overview of uniwig and the universe creation tools.]
) <gtokenizers-uniwig>
#figure-caption-extended(caption: [
*a.* Schemtic overview of the uniwig process. The tool takes as input a set of BED files and will output coverage tracks for the cores starts and ends. *b.* Coverage-based universes are derived from the genome coverage of a collection of region sets. Examples include intersection $U_i$, coverage cutoff $U_"CC"$, and union universe $U_"union"$. *c.* A flexible region in contrast to fixed region can represent boundaries of many variable regions. *d.* The flexible coverage cutoff (CCF) universe is based on coverage of the genome by a collection. It uses two cutoff values: the lower defines flexible boundaries and the upper defines the region core. *e.* A collection of genomic region sets is aggregated, and region starts, core (overlap), and ends are counted, creating signal tracks. *.f* Maximum likelihood universe is derived from three signal tracks. Using a likelihood model, we build a scoring matrix that assesses the probability of each position being a given part of a flexible region. Next, we find the most likely path, which represents the maximum likelihood universe. *g.* The HMM universe treats signal tracks representing genome coverage by different parts of a region as emissions of hidden states that correspond to different parts of flexible regions.
])

=== Creating a principled vocabulary for genomic intervals
The creation of a principled vocabulary for genomic intervals requires a systematic two-step process. First, input genomic interval datasets (typically BED files) are processed using `uniwig` to generate standardized coverage tracks representing interval starts, cores, and ends across the genome. Second, these coverage tracks are then processed through one of four universe construction methods—coverage cutoff (CC), flexible coverage cutoff (CCF), maximum likelihood (LH), or Hidden Markov Model (HMM)—to generate the final consensus genomic interval set that serves as the vocabulary for machine learning applications (@gtokenizers-uniwig\A, #link(<infrastructure-uniwig-methods>, "See methods")).

=== Overview of uniwig: a pre-processing tool for consensus genomic interval set construction
The first step when building a machine learning model for genomic intervals is to define a universe of intervals that will serve as the vocabulary for the model. This universe should comprehensively cover the genomic regions relevant to the biological context of interest, such as regulatory elements in a specific cell type or tissue. The universe is typically represented as a BED file containing genomic coordinates. Once the universe is defined, new datasets can be tokenized by mapping their intervals to the nearest or overlapping regions in the universe, converting them into a format suitable for machine learning models.

To facilitate this process, we developed `uniwig`, a high-performance tool in rust that computes the coverage of genomic intervals across multiple datasets and generates a unified representation. `uniwig` takes as input a set of BED files representing genomic intervals from different experiments and computes three single coverage `.bigWig` files, one for the starts, the cores, and the ends of the intervals. This representation captures the distribution of intervals across the genome and can be used to identify regions that are consistently covered across datasets. The output `.bigWig` files can then be used as input for downstream analysis or as part of the vocabulary creation process (@gtokenizers-uniwig\A, #link(<infrastructure-uniwig-methods>, "See methods")).

=== Simple coverage-based universe construction
Once the coverage files are generated, they can be used to create a consensus set of genomic intervals that will serve as the vocabulary for machine learning models. The simplest method for constructing a consensus genomic interval set is to use coverage information directly. This approach involves computing the coverage of genomic intervals across all input datasets and applying a threshold to define which regions should be included in the final universe. One can apply either a fixed coverage cutoff (CC), an intersection, or a union of all input datasets to define the universe (@gtokenizers-uniwig\B). However, one limitation of this approach is that it treats each interval as a binary event (present or absent), which may not capture the full complexity of the data, especially when intervals vary in length or when there are differences in the number of intervals across datasets. For this, we consider methods that consider flexible regions -- that is, regions with a start, core, and end -- to better capture the underlying biological signals (@gtokenizers-uniwig\C).

=== Novel methods for constructing consensus genomic interval sets
To improve upon simple coverage-based approaches, several advanced methods have been developed within our group, with the definitive framework and evaluation published by Rymuza _et al._ (2024). In summary, we provide three methods for generating these consensus sets, each with its own advantages and trade-offs: 1) coverage cutoff (CC) universe, 2) maximum likelihood (LH) universe, and 3) the HMM universe. These methods were developed by our group and others @Rymuza2024, and each is suited to different scenarios depending on the characteristics of the input data and the desired properties of the resulting universe. We briefly summarize each method below (#link(<infrastructure-universe-methods>, "See methods for more details")).

==== Coverage Cutoff Flexible (CCF) Universe
This method treats a collection of flexible genomic region sets as a single coverage signal track across the genome. It then applies a coverage threshold, or cutoff, to this track. Any genomic position where the coverage is greater than or equal to the chosen cutoff is included in the final universe. This approach is straightforward and allows for easy control over the size of the universe by adjusting the cutoff value. However, it may not capture all relevant regions, especially if the input datasets are highly variable or if some regions are only present in a few datasets (@gtokenizers-uniwig\D, @gtokenizers-uniwig\E). This approach is a hybrid between simply taking the union of all regions (a cutoff of 1) and taking the intersection (a cutoff equal to the total number of region sets). The method provides a tunable parameter that can be adjusted based on the needs of the analysis.

==== Maximum Likelihood (LH) Universe
The Maximum Likelihood (LH) universe was designed to better preserve the boundaries of individual regions and avoid merging adjacent regions, which can be an issue with simple coverage-based methods. Instead of just using coverage, this method calculates three separate signal tracks at base-pair resolution: one for region starts, one for region ends, and one for coverage (core). It then uses a likelihood model to calculate the probability of each genomic position being a start, core, or end (@gtokenizers-uniwig\F). The final universe is determined by finding the most likely path of these states (start, core, end) across the genome.

==== Hidden Markov Model (HMM) Universe
Lastly, the Hidden Markov Model (HMM) approach is a more tunable and sophisticated version of the likelihood model. It models the genome as a sequence of hidden states (e.g., region start, core, end, or background). The three signal tracks (starts, ends, coverage) are treated as emissions, or observations, generated by these hidden states. The key advantage is that a user can adjust the model's transition probabilities (the likelihood of moving from one state to another) and emission probabilities to fine-tune the resulting universe, for example, to prevent unnecessary fragmentation of regions (@gtokenizers-uniwig\G).

This framework established by Rymuza _et al._ @Rymuza2024 provides a systematic approach to constructing consensus genomic interval sets that can be tailored to the specific needs of different machine learning applications. By leveraging the coverage information from multiple datasets, these methods enable the creation of robust and biologically meaningful vocabularies that facilitate the application of machine learning techniques to genomic interval data.

Building on this comprehensive evaluation framework, Rymuza _et al._ @Rymuza2024 conducted systematic benchmarking across multiple genomic datasets to assess the performance of these universe construction methods in downstream machine learning applications. Their analysis revealed that the HMM universe consistently outperformed simpler coverage-based approaches and the maximum likelihood method across various evaluation metrics, including model accuracy, feature interpretability, and biological relevance of the resulting vocabularies (@universe-results-overview).

Having established methods for creating consensus vocabularies, the next challenge is efficiently mapping new genomic datasets to these standardized feature spaces. This process — analogous to tokenization in natural language processing — requires specialized tools optimized for the scale and performance demands of modern machine learning workflows.

#figure(
  image("/fig/gtokenizers/overview.svg"),
  caption: [Overview and benchmarking of gtokenizers, a Rust-based library for genomic interval tokenization.]
) <gtokenizers-overview>
#figure-caption-extended(caption:
[
  *a*, Schematic of natural language tokenization. NLP tokenizers typically break sentences up into words or word-pieces. *b*, Schematic illustrating gtokenizers applied to regulatory elements (e.g., cCREs) for standardized interval representation. *c*, Architecture of gtokenizers, with a core implementation in Rust and support for multiple language bindings (e.g., CLI, R, Python, WebAssembly). *d*, Runtime benchmarking across three query sizes (1M, 100K, 10K regions) against existing tools (bedtools, bedops, bedtk) and Rust-based implementations (`gtars/bits`, `gtars/alist`), demonstrating scalability and performance.
])

=== Overview of genomic interval tokenizers
Modern deep‑learning workflows in natural language processing require tokenizers to convert new text into the model’s fixed vocabulary, enabling consistent inputs for downstream processing. Tokens in language models correspond to discrete words or subword units (@gtokenizers-overview\A). In the genomic domain, a comparable process is necessary: machine learning models that treat genomic intervals as discrete units, like words in a sentence, must map each dataset to a common set of regions, or a vocabulary for genomic intervals @Gharavi2021 @Gharavi2024 @Zheng2024 @Rymuza2024 @LeRoy2024 This vocabulary ensures data across experiments are represented in a standardized, comparable way (@gtokenizers-overview\B). Different datasets can thus be interpreted with the same model architecture and feature space, just as diverse text inputs are aligned via tokenization in NLP.

We implemented two overlap methods in `gtars-tokenizers`: `gtars/bits`, which uses binary interval tree search (BITS) @Layer2013, and `gtars/alist`, which uses an Augmented Interval List (AIList) @Feng2019 (#link(<infrastructure-tokenization-methods>, "see methods")). Both methods are implemented in Rust for performance and memory efficiency. To maximize flexibility and usability, we provide bindings for `gtars/tokenizers` in Python, R, and WebAssembly, as well as a command-line interface (CLI) (@gtokenizers-overview\C). This allows users to integrate genomic interval tokenization into their existing workflows, whether they are using Python-based machine learning libraries like TensorFlow or PyTorch, R-based bioinformatics tools, or require a web-based solution for use in a browser (#link(<infrastructure-environment-bindings>, "see methods")).

=== Gtars tokenizers are highly performant
To highlight the performance of `gtars/tokenizers`, we benchmarked it against existing tools for genomic interval tokenization (#link(<infrastructure-tokenization-benchmarking>, "see methods")). We compare `gtars/tokenizers` to `bedtools`, `bedops`, and `bedtk` @Quinlan2010 @Neph2012 @Li2021a. These tools focus on general-purpose genomic interval arithmetic and are not optimized for machine learning applications. We found `gtars/tokenizers` to be consistently as fast or faster than existing tools (@gtokenizers-overview\D). For large universes with >1 million intervals (like those used in genomic interval machine learning), `gtars-tokenizers` is around 2- 3x faster than `bedtools` and `bedops`, while being comparable to `bedtk`. This pattern holds across different query sizes (1M, 100K, and 10K regions), demonstrating the scalability and performance of `gtars/tokenizers`.

=== Gtars tokenizers work seamlessly with modern machine learning infrastructure
The gtars-tokenizer implementation is compatible with the Hugging Face tokenizers API, enabling seamless integration with the broader Hugging Face ecosystem. The `gtars` tokenizers are near-drop-in replacements for existing Hugging Face tokenizers, meaning users can pass them to the HuggingFace transformers package functions and classes using the same ergonomics as a standard NLP workflow. The consistent interface makes it easy for ML engineers to adapt to training models on genomic interval data. It also means that the downstream outputs of the training process will seamlessly integrate with popular downstream frameworks and tools that rely on the Hugging Face tokenizers standard, such as PyTorch Lightning, AllenNLP, and evaluation libraries like Evaluate, PEFT, and Weights & Biases. To highlight this, we provide a brief example of how someone can use our tokenizers to preprocess data for a simple neural network built with PyTorch. The snippet first creates a new tokenizer from a BED-file, and then uses it to preprocess data for a neural network.

```python
import torch
import gtars.tokenizers as Tokenizer

tokenizer = Tokenizer.from_bed("path/to/bed/file.bed")
network = torch.nn.Embedding(tokenizer.vocab_size, 64)

query_intervals = [("chr1", 1000, 2000), ("chr2", 3000, 4000)]
tokens = tokenizer.tokenize(query_intervals)["input_ids"]
out = network(torch.tensor(tokens))
```

=== Gtars tokenizers are available in a wide array of computing environments.
To maximize usability, we expose the Rust core of gtars-tokenizers as a Rust library crate, as a command-line tool, with R bindings, Python bindings, and for WebAssembly (WASM). This broad set of interfaces ensures that the same high-performance engine can serve diverse communities -- from machine learning researchers to bioinformaticians and end-users in web tools -- without duplicating functionality or compromising performance. It also reduces maintenance requirements for the community because a single fast interface can be deployed in many situations.

