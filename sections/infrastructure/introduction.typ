Advancements in high-throughput sequencing technologies have generated vast and diverse epigenomic datasets from assays such as ChIP-seq, ATAC-seq, and Hi-C @Sheffield2012. These experiments are frequently summarized as genomic intervals, which define regions on a genome. Summarized genomic interval data has grown rapidly over the past few years @Xue2023. This proliferation of data provides a critical opportunity to uncover generalizable patterns, support predictive modeling, and enable transfer learning using large-scale machine learning (ML) methods. However, major barriers in applying modern ML methods to genomic interval data arise due to two fundamental gaps in available tooling: 1) the lack of systematic methods for creating consensus vocabularies of genomic intervals (vocabulary builders), and 2) the absence of efficient, flexible tools for mapping new datasets to these vocabularies (tokenizers).

First, genomic interval data is inherently variable and unstructured; each dataset defines its own regions of interest, making it difficult to compare or combine results across experiments. This is incompatible with ML methods, which generally require data to be described in a discrete, consistent vocabulary. For example, in natural language processing (NLP), models require well-defined vocabularies to process and integrate diverse sets of textual data. The process of mapping new, unseen datasets to a shared feature set is called tokenization and is a vital part of NLP research and development @Sennrich2016 @Wu2016 @Kudo2018 @Kudo2018a. Without such a standardized basis, it is difficult or impossible to create feature-aligned representations suitable for ML. Similarly, for genomic intervals, it is necessary to map new datasets to a shared vocabulary, or consensus set of genomic intervals @Xue2023 @Rymuza2024. This process is conceptually similar to tokenization in NLP and serves the same purpose: to enable consistent and scalable representation of variable input data. However, creating these consensus vocabularies remains largely ad hoc and manual. While methods exist for generating consensus peak sets or reference interval collections, they are typically designed for specific analyses rather than systematic vocabulary creation for ML applications. Most approaches rely on simple binning, intersection or merging strategies @Chen2019 @Sheffield2016 @Yan2020 without considering the downstream requirements of machine learning models, such as vocabulary size constraints, feature importance, or cross-dataset generalizability. This gap leaves researchers to develop custom, often suboptimal solutions for each project.

Second, even when consensus vocabularies exist, the tools for efficiently mapping genomic intervals to these shared feature sets are inadequate for modern ML workflows. Specifically, while tools exist for genomic interval comparison @Quinlan2010 @Neph2012 @Li2021a @Feng2019 @Feng2021, they are limited in several ways. They are typically only accessible in a single environment, such as R, or as command-line tools, and are not optimized for fast, in-memory processing. This limitation poses a significant pain point for machine learning pipelines in Python, which require high-throughput, efficient data handling. Additionally, they generally lack flexible APIs that integrate seamlessly in the Python-based machine learning ecosystem, particularly with libraries like PyTorch, TensorFlow, or huggingface/transformers. As a result, ML applications in genomics often suffer from ad hoc preprocessing steps, pipeline bottlenecks, and limited scalability.

To solve these problems, we created `gtars`, a comprehensive library designed specifically for genomic machine learning that addresses both gaps, and we created `geniml`, a toolkit with capabilities for generating consensus peak sets for genomic machine learning models. For vocabulary creation, we provide `gtars-uniwig` (referred to as `uniwig`), a pre-processing tool for systematic consensus set generation that serves as the first necessary and compute-intensive preprocessing step for our downstream vocabulary builders. For tokenization, we developed `gtars-tokenizers`, which provides four main improvements: First, its Rust core makes it faster than many existing tools, and in many cases, as fast as the fastest available implementations. Second, it is designed for convenience for ML, exposing a direct bridge into modern ML infrastructure such as HuggingFace and PyTorch, so genomic intervals can be tokenized and passed into models without ad hoc preprocessing. Third, unlike prior utilities, it treats genomic intervals in a way that mirrors the conceptualization of words in NLP, enabling consistent, vocabulary-based representations that scale across datasets. Finally, it offers a unified engine with bindings for Python, R, Rust, command line, and web applications so the same foundation can serve diverse users and workflows.

These tools are essential components for establishing robust and modern ML workflows for genomic interval data. The standardization and efficiency they provide are critical for scaling genomic machine learning beyond individual experiments to large-scale, multi-dataset analyses. By addressing both vocabulary creation and tokenization systematically, `gtars` enables researchers to focus on model development and biological insights rather than wrestling with data preprocessing challenges. In the following sections, we describe these tools in turn, demonstrating how they work together to bridge the gap between genomic data and machine learning infrastructure.