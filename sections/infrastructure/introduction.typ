Advancements in high-throughput sequencing technologies have generated vast and diverse epigenomic datasets from assays such as ChIP-seq, ATAC-seq, and Hi-C @Sheffield2012. These experiments are frequently summarized as genomic intervals, which define regions on a genome. Summarized genomic interval data has grown rapidly over the past few years @Xue2023. This proliferation of data provides a critical opportunity to uncover generalizable patterns, support predictive modeling, and enable transfer learning using large-scale machine learning (ML) methods. However, a major barrier in applying modern ML methods to genomic interval data arises due to the heterogeneity of the data. Genomic interval data is inherently variable and unstructured; each dataset defines its own regions of interest, making it difficult to compare or combine results across experiments. This is incompatible with ML methods, which generally require data to be described in a discrete, consistent vocabulary. For example, in natural language processing (NLP), models require well-defined vocabularies to process and integrate diverse sets of textual data. The process of mapping new, unseen datasets to a shared feature set is called tokenization and is a vital part of NLP research and development @Sennrich2016 @Wu2016 @Kudo2018 @Kudo2018a. Without such a standardized basis, it is difficult or impossible to create feature-aligned representations suitable for ML. Similarly, for genomic intervals, it is necessary to map new datasets to a shared vocabulary, or consensus set of genomic intervals @Xue2023 @Rymuza2024. This process is conceptually similar to tokenization in NLP. It serves the same purpose: to enable consistent and scalable representation of variable input data.

The basis of mapping genomic intervals into a shared, consensus set lies in general-purpose interval comparison. While many tools exist for genomic interval comparison @Quinlan2010 @Neph2012 @Li2021a @Feng2019 @Feng2021, they are limited in several ways. First, they are typically only accessible in a single environment, such as R, or as command-line tools. They are not optimized for fast, in-memory processing. This limitation poses a significant pain point for machine learning pipelines in Python, which require high-throughput, efficient data handling. Second, they generally lack flexible APIs that integrate seamlessly in the Python-based machine learning ecosystem, particularly with libraries like PyTorch, TensorFlow, or huggingface/transformers. As a result, ML applications in genomics often suffer from ad hoc preprocessing steps, pipeline bottlenecks, and limited scalability.

To solve these problems, we created `gtars-tokenizers`, a library designed specifically for genomic machine learning. It provides four main improvements: First, its Rust core makes it faster than many existing tools, and in many cases, as fast as the fastest available implementations. Second, it is designed for convenience for ML, exposing a direct bridge into modern ML infrastructure such as HuggingFace and PyTorch, so genomic intervals can be tokenized and passed into models without ad hoc preprocessing. Third, unlike prior utilities, it treats genomic intervals in a way that mirrors the conceptualization of words in NLP, enabling consistent, vocabulary-based representations that scale across datasets. Finally, it offers a unified engine with bindings for Python, R, Rust, command line, and web applications so the same foundation can serve diverse users and workflows.
