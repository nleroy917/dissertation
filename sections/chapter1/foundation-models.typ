=== Foundation models in genomics
One of the most exciting developments in machine learning has been the emergence of large, pre-trained models that can be fine-tuned for specific tasks with relatively little additional data. These models, often referred to as "foundation models," have demonstrated remarkable performance across a wide range of applications, from image recognition to natural language understanding @Devlin2019a @Radford2018 @Radford2021a @Brown2020 @Raffel2023 @Dosovitskiy2021 @Touvron2023 @He2015.

The emergence of foundation models in CV and NLP has inspired similar efforts in the field of genomics. Typically built on the transformer architecture @Vaswani2017, these models are pre-trained on large-scale genomic datasets and can be used in a variety of downstream tasks for zero-shot predictions, few-shot learning, and transfer learning. Examples include DNABERT @Ji2021: bidirectional encoder for DNA sequences, Enformer: a model that predicts gene expression from DNA sequence @Avsec2021, AlphaFold: a model for predicting protein structures from amino acid sequences @Abramson2024 @Jumper2021 @Senior2020, DNA Discrete Diffusion: a model for generating DNA sequences with desired properties @Sarkar2024, scGPT: a transformer-based model for single-cell RNA-seq data @Cui2024, and Geneformer: a transformer-encoder model for scRNA-seq data @Theodoris2023. Overwhelmingly, these models will take advantage of transfer learning to apply them to new, unseen datasets and tasks. This makes them particularly attractive as they can be fine-tuned with relatively little data and computational resources compared to training a model from scratch which is often infeasible for many researchers.

=== Foundation models for single-cell ATAC-seq data
While many models exist for DNA sequences, protein sequences, and scRNA-seq data, until very recently, there has been a lack of similar efforts focused on scATAC-seq data. This is likely due to the unique challenges posed by scATAC-seq data, including its high dimensionality, sparsity, and the complex relationship between chromatin accessibility and gene regulation. Moreover, scATAC-seq data is less ubiquitous than other types of genomic data, making it much more difficult to obtain large, diverse training datasets.

However, this landscape has begun to change with the recent emergence of foundation models specifically designed for chromatin accessibility data. Within the last six months, two notable methods have been introduced: EpiAgent @Chen2024 and ChromFound @Jiao2025. These models represent the first serious attempts to develop large-scale, pre-trained foundation models for scATAC-seq analysis, potentially addressing many of the limitations faced by previous approaches while leveraging the power of modern deep learning architectures.

=== A note on tokenization
Tokenization is a critical first step for all foundation models, as it defines how raw inputs are mapped into a shared feature space that the model can operate on. In natural language processing, this is achieved through subword vocabularies that segment text into units like words or byte-pair encodings, creating a stable and universal representation of language @Wu2016 @Kudo2018 @Kudo2018a. In computer vision, images are typically partitioned into fixed-size patches that act as tokens, enabling transformers to process them as sequences.

Genomics has followed suit: proteomics and transcriptomics benefit from clear, biologically grounded vocabularies—amino acids for proteins, nucleotides or k-mers for RNA—supported by established gene ontologies. By contrast, single-cell ATAC-seq presents a uniquely difficult tokenization problem: _the natural unit of information, a genomic interval, is not standardized across experiments or datasets, and “shared vocabularies” can vary dramatically depending on how peaks are called or which reference regions are chosen_. This lack of a canonical token set has delayed the emergence of foundation models for scATAC-seq, as the success of such models depends on a stable, unified representation of input features across diverse datasets.



=== Limitations of current EpiAgent and ChromFound
EpiAgent is a transformer-based foundation model trained on ~5 million single cells or ~35 billion tokens from 31 tissues. The model has about 1.4 billion parameters, divided between an embedding module, an 18-layer bidirectional transformer, and a signal decoder. EpiAgent tokenizes cells by converting each cell’s accessible cCREs into a “cell sentence,” ranking cCREs by TF-IDF values and limiting input length to ~8,190 tokens￼. EpiAgent uses two novel training tasks for pre-training: cell-cCRE alignment (classifying whether cCREs are accessible given the cell embedding) and signal reconstruction (rebuilding accessibility signals from embeddings). The model outputs contextualized cCRE embeddings and a [CLS] cell embedding that capture cellular heterogeneity and regulatory networks. These representations can be adapted for downstream tasks like unsupervised feature extraction, supervised annotation, data imputation, perturbation prediction, batch correction, query mapping, and even in-silico cCRE knockout analysis 

ChromFound, similarly encodes single-cells into a vector representation.

While both methods have shown promise and broken new ground in the field of scATAC-seq, they still suffer from limitations and pitfalls. First, these methods rely on large, cumbersome models that exceed 1 billion parameters. This scale rivals even some large language models today making them computationally intractable even with significant compute and GPU resources. Second, these models lack accessibility and reusability. ChromFound, to date, is still closed source making it impossible to actually use. EpiAgent, while open source, lacks a user-friendly system for accessing pre-trained models. The convention in CV and NLP is to share these models through huggingface, which neither EpiAgent or ChromFound do. This also makes fine-tuning these models on new data near impossible. Finally, and most importantly, these models lack interpretability. Tokenized cells are represented as dense, high-dimensional vectors which lack explainability.

These limitations leave a lot to be desired when it comes to large, pre-trained foundation models for scATAC-sea data. The Atacformer project addresses each of these in turn. First it's designed to be efficient: Atacformer uses almost 1/10th the number of parameters making it significantly faster and memory-efficient while maintaining comparable performance. It achieves this through the use of satte-of-the-art efficient attention mechanisms @Dao2022a @Dao2023. Second, it was designed to be accessible. Built on top of the `transformers` library, Atacformer models can be instantiated, fine-tuned, and expanded using very few lines of code. This fosters community engagement and 
