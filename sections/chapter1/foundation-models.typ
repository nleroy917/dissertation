One of the most exciting developments in machine learning has been the emergence of large, pre-trained models that can be fine-tuned for specific tasks with relatively little additional data. These models, often referred to as "foundation models," have demonstrated remarkable performance across a wide range of applications, from image recognition to natural language understanding @Devlin2019a @Radford2018 @Radford2021a @Brown2020 @Raffel2023 @Dosovitskiy2021 @Touvron2023 @He2015.

The emergence of foundation models in CV and NLP has inspired similar efforts in the field of genomics. Typically built on the transformer architecture @Vaswani2017, these models are pre-trained on large-scale genomic datasets and can be used in a variety of downstream tasks for zero-shot predictions, few-shot learning, and transfer learning. Examples include DNABERT @Ji2021: bidirectional encoder for DNA sequences, Enformer: a model that predicts gene expression from DNA sequence @Avsec2021, AlphaFold: a model for predicting protein structures from amino acid sequences @Abramson2024 @Jumper2021 @Senior2020, DNA Discrete Diffusion: a model for generating DNA sequences with desired properties @Sarkar2024, scGPT: a transformer-based model for single-cell RNA-seq data @Cui2024, and Geneformer: a transformer-encoder model for scRNA-seq data @Theodoris2023. Overwhelmingly, these models will take advantage of transfer learning to adapt to the specific characteristics of scATAC-seq data, potentially improving performance and reducing the need for large labeled datasets.

While many models exist for DNA sequences, protein sequences, and scRNA-seq data, until very recently, there has been a lack of similar efforts focused on scATAC-seq data. This is likely due to the unique challenges posed by scATAC-seq data, including its high dimensionality, sparsity, and the complex relationship between chromatin accessibility and gene regulation. Moreover, scATAC-seq data is less ubiquitous than other types of genomic data, making it much more difficult to obtain large, diverse training datasets.

However, this landscape has begun to change with the recent emergence of foundation models specifically designed for chromatin accessibility data. Within the last six months, two notable methods have been introduced: EpiAgent @Chen2024 and ChromFound @Jiao2025. These models represent the first serious attempts to develop large-scale, pre-trained foundation models for scATAC-seq analysis, potentially addressing many of the limitations faced by previous approaches while leveraging the power of modern deep learning architectures.

=== Limitations of current EpiAgent and ChromFound
EpiAgent is a pre-trained transformer encoder model trained on over 5 million cells. 

ChromFound, similarly encodes single-cells into a vector representationation

While both methods have shown promise and broken new ground in the field of scATAC-seq, they still suffer from limitations and pitfalls. First, these methods rely on large, cumbersome models that exceed 1 billion parameters. This scale rivals even some large language models today making them computationally intractable even with significant compute and GPU resources. Second, these models lack accessibility and reusability. ChromFound, to date, is still closed source making it impossible to actually use. EpiAgent, while open source, lacks a user-friendly system for accessing pre-trained models. The convention in CV and NLP is to share these models through huggingface, which neither EpiAgent or ChromFound do. This also makes fine-tuning these models on new data near impossible. Finally, and most importantly, these models lack interpretability. Tokenized cells are represented as dense, high-dimensional vectors which lack explainability.

These limitations leave a lot to be desired when it comes to large, pre-trained foundation models for scATAC-sea data. The Atacformer project addresses each of these in turn. First it's designed to be efficient: Atacformer uses almost 1/10th the number of parameters making it significantly faster and memory-efficient while maintaining comparable performance. It achieves this through the use of satte-of-the-art efficient attention mechanisms @Dao2022a @Dao2023. Second, it was designed to be accessible. Built on top of the `transformers` library, Atacformer models can be instantiated, fine-tuned, and expanded using very few lines of code. This fosters community engagement and 
