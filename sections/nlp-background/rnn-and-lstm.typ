=== A river bank or a financial bank? Contextual embeddings and their limitations
One of the critical assumptions made with Word2Vec and other word embedding models is that word embeddings are context-_independent_; that is, each word has a single fixed representation regardless of its surrounding context. In other words, the embedding for a word like "bank" would be the same whether it appears in the context of a financial institution or the side of a river. The word is simply "looked up" in the embedding table and used as-is (@w2v-rnn-transformer\A). While this simplification has been effective for many NLP tasks, it limits the model's ability to capture the nuanced meanings of words that can change based on context. Instead the embedding for "bank" will be some average of the two meanings. This limitation motivated the development of contextual embeddings, which emerged first through recurrent neural networks and later evolved into the attention-based architectures that dominate today.

=== Recurrent Neural Networks (RNNs)
Recurrent Neural Networks (RNNs) are a class of neural networks designed for processing sequential data, making them particularly well-suited for tasks in natural language processing (NLP). Unlike traditional feedforward neural networks, RNNs have connections that loop back on themselves, allowing them to maintain a hidden state that can capture and store information about previous inputs in the sequence. This makes RNNs capable of modeling temporal dependencies and context, which are crucial for understanding language.

In a standard RNN architecture, words are still represented using embeddings, such as those learned by Word2Vec. However, instead of treating each word independently, the RNN processes the sequence of word embeddings one at a time, updating its hidden state at each step. The hidden state serves as a memory that captures information about the words that have been processed so far. This allows the RNN to generate context-aware representations of words based on their surrounding context in the sequence (@w2v-rnn-transformer\B).

Because of this, now if the word "bank" appears in a sentence, the RNN can use its hidden state to determine whether the context suggests a financial institution or the side of a river, and adjust its representation accordingly. This ability to capture context makes RNNs more powerful than static word embeddings for many NLP tasks. Still, simple RNNs often struggled with longer sequences due to unstable training dynamics, motivating a series of refinements which we describe next.


=== Long Short-Term Memory (LSTM) networks
While RNNs are capable of modeling sequential data, they suffer from a significant limitation known as the *vanishing gradient problem*. This issue arises when training RNNs on long sequences, where the gradients used for updating the model's weights can become very small, effectively preventing the model from learning long-range dependencies in the data. Intuitively, this means that the RNN may struggle to "remember" information from earlier in the sequence when making predictions about later words because the influence of those earlier words diminishes significantly over time. If we are discussing a financial bank, but the word "bank" appears several words later, the model may have difficulty connecting the two concepts.

To address this limitation, Long Short-Term Memory (LSTM) networks were introduced by Hochreiter and Schmidhuber in 1997. LSTMs are a specialized type of RNN that incorporate a more complex architecture designed to better capture long-range dependencies in sequential data. The key innovation of LSTMs is the introduction of memory cells and gating mechanisms that regulate the flow of information into and out of these cells. These features enabled dramatic improvements in applications such as speech recognition and handwriting generation.

Shortly afterward, simplified variants such as the Gated Recurrent Unit (GRU) were proposed to retain much of the performance of LSTMs while reducing computational cost @Cho2014. Building on these architectures, the *sequence-to-sequence (seq2seq)* @Sutskever2014 framework demonstrated how paired encoderâ€“decoder RNNs could perform end-to-end translation, and attention mechanisms were layered on top to further improve performance @Bahdanau2016a. These developments marked the high-water mark of RNN-based NLP systems before the shift toward fully attention-based models.

Even still, LSTMs and GRUs have their own limitations. They can be computationally intensive to train, especially on very long sequences, and they may still struggle with extremely long-range dependencies. Additionally, while RNN-based architectures improved upon static embeddings, they were ultimately surpassed by transformer models, which dispense with recurrence entirely in favor of self-attention. We turn to those next.