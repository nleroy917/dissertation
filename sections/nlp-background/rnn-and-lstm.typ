=== A river bank or a financial bank? Contextual embeddings and their limitations
One of the critical assumptions made with Word2Vec and other word embedding models is that word embeddings are context-_independent_; that is, each word has a single fixed representation regardless of its surrounding context. In other words, the embedding for a word like "bank" would be the same whether it appears in the context of a financial institution or the side of a river. The word is simply "looked up" in the embedding table and used as-is (@w2v-rnn-transformer\A). While this simplification has been effective for many NLP tasks, it limits the model's ability to capture the nuanced meanings of words that can change based on context. Instead the embedding for "bank" will be some average of the two meanings. This limitation motivated the development of contextual embeddings, which emerged first through recurrent neural networks and later evolved into the attention-based architectures that dominate today.

=== Recurrent Neural Networks (RNNs)
Recurrent Neural Networks (RNNs) are a class of neural networks designed for processing sequential data, making them particularly well-suited for tasks in natural language processing (NLP). Unlike traditional feedforward neural networks, RNNs have connections that loop back on themselves, allowing them to maintain a hidden state that captures and stores information about previous inputs in the sequence. This makes RNNs capable of modeling temporal dependencies and context, which are crucial for understanding language.

In a standard RNN architecture, words are still represented using embeddings, such as those learned by Word2Vec. However, instead of treating each word independently, the RNN processes the sequence of word embeddings one at a time, updating its hidden state at each step. The hidden state serves as a memory that captures information about the words that have been processed so far. This allows the RNN to generate context-aware representations of words based on their surrounding context in the sequence (@w2v-rnn-transformer\B).

Because of this, now if the word "bank" appears in a sentence, the RNN can use its hidden state to determine whether the context suggests a financial institution or the side of a river, and adjust its representation accordingly. This ability to capture context makes RNNs more powerful than static word embeddings for many NLP tasks. Still, simple RNNs often struggled with longer sequences due to unstable training dynamics, motivating a series of refinements which we describe next.


=== Long Short-Term Memory (LSTM) networks
While RNNs are capable of modeling sequential data, they suffer from a significant limitation known as the *vanishing gradient problem*. This issue arises when training RNNs on long sequences, where the gradients used for updating the model's weights can become very small, effectively preventing the model from learning long-range dependencies in the data. Intuitively, this means that the RNN may struggle to "remember" information from earlier in the sequence when making predictions about later words because the influence of those earlier words diminishes significantly over time. If we are discussing a financial bank, but the word "bank" appears several words later, the model may have difficulty connecting the two concepts.

To address this limitation, the NLP community turned to an architecture that had been introduced much earlier: the Long Short-Term Memory (LSTM) network (Hochreiter & Schmidhuber, 1997). While the original idea dates to the late 1990s, LSTMs gained prominence in the 2010s as sufficient computational power became available to train them effectively and the vanishing gradient problem proved a major bottleneck for simpler RNNs.

LSTMs are a specialized type of RNN that incorporate a more complex architecture designed to better capture long-range dependencies. The key innovation is the introduction of a memory cell and gating mechanisms (input, output, and forget gates) that regulate the flow of information. These gates allow the network to selectively remember or forget information over long periods, largely overcoming the vanishing gradient problem that plagued simple RNNs. Intuitively, this means an LSTM can hold on to the "financial" context long enough to correctly interpret the word "bank" when it appears much later in a sequence.

The ability to effectively model long-range dependencies unlocked the potential of recurrent models, leading to dramatic improvements in applications like machine translation and speech recognition. The success of LSTMs spurred further innovation. Simplified variants, such as the Gated Recurrent Unit (GRU), were proposed to retain much of the performance of LSTMs while reducing computational cost @Cho2014. Building on these architectures, the sequence-to-sequence (seq2seq) framework demonstrated how paired encoder–decoder RNNs could perform end-to-end translation @Sutskever2014, and attention mechanisms were layered on top to further improve performance @Bahdanau2016a. These developments marked the high-water mark of RNN-based NLP systems before the shift toward fully attention-based models.

The success of RNNs and LSTMs in natural language processing naturally extended beyond text to other sequential data types, particularly in computational biology. Researchers recognized that biological sequences — those like DNA, RNA, and protein sequences — share fundamental similarities with natural language: they are composed of discrete symbols (nucleotides or amino acids) arranged in meaningful sequences where order and context matter significantly. This analogy proved fruitful, leading to the application of LSTM architectures for tasks such as gene expression prediction, protein function annotation, and DNA sequence classification @Min2017 @Wang2019.

Even still, LSTMs and GRUs have their own limitations. They can be computationally intensive to train, especially on very long sequences, and they may still struggle with extremely long-range dependencies. Additionally, while RNN-based architectures improved upon static embeddings, they were ultimately surpassed by transformer models, which dispense with recurrence entirely in favor of self-attention. We turn to those next.