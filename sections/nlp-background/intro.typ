Natural language processing (NLP) has long served as both a proving ground and a catalyst for innovations in machine learning. The field’s central challenge — learning meaningful representations from sequential data — closely parallels problems found in domains like biology, where genomic sequences or chromatin accessibility profiles can be viewed as “languages.” Over the past two decades, NLP has undergone a dramatic shift: from early statistical models based on word counts, to distributed word embeddings like Word2Vec, to recurrent neural networks and their gated variants, and finally to transformers, which have redefined what is possible in sequence modeling. Tracing this trajectory not only illuminates the conceptual breakthroughs that led to today’s foundation models, but also provides the intellectual scaffolding for adapting these methods to biological data such as scATAC-seq.

This chapter provides a concise overview of key developments in NLP that have shaped modern machine learning as well as our own work on genomc interval analysis. We will, in turn, discuss the evolution of word embeddings, the advent of attention mechanisms and transformers, and the rise of large pre-trained models. By understanding these milestones, we can better appreciate how techniques originally designed for human language can be repurposed to decode the “language” of genome regulation.