=== Future aim 4: Context window optimization — exploring the extremes
==== Motivation
Throughout Atacformer development, we employed a fixed context window of 8,192 tokens, chosen to accommodate the majority of single cells while remaining computationally tractable. However, this one-size-fits-all approach leaves substantial room for optimization in _both_ directions. Preliminary analysis revealed that while many cells benefit from large context windows, a significant proportion may be adequately represented with far fewer tokens (@atacformer-context-window-analysis), particularly when coupled with improved tokenization strategies that prioritize the most informative regions. Conversely, our ability to analyze bulk ATAC-seq data remains severely limited, as bulk datasets often contain 50,000–200,000 accessible regions per sample, far exceeding our current capacity. Exploring both extremes—smaller context windows for efficiency and larger windows for comprehensive genomic coverage—would unlock new capabilities and broaden the applicability of foundation models in regulatory genomics.

==== Proposed approach: Scaling down with efficient architectures
The first direction involves determining how small context windows can be while maintaining performance. This could be achieved by combining tokenization strategies with importance scoring to retain only the most informative regions. Systematic experimentation with reduced context sizes (e.g., 512, 1,024, 2,048 tokens) could identify the optimal trade-off between biological coverage and computational efficiency. This could be paired with advanced self-attention implementations such as state space models (Mamba, S4) @Gu2024 @Dao2022 that offer subquadratic scaling with sequence length, dramatically reducing training and inference time. Additional efficiency gains could come from model compression techniques like quantization to create lightweight variants suitable for CPU-based inference. The goal would be to produce models analogous to the sentence-transformer family in natural language processing @Reimers2019: fast, efficient, and deployable in local environments without specialized hardware, enabling researchers with limited computational resources to leverage foundation model representations.

==== Proposed approach: Scaling up for bulk data integration
The second direction involves extending context windows to accommodate comprehensive bulk ATAC-seq datasets, which contain orders of magnitude more accessible regions than single-cell data. Our initial bulk data analysis was constrained to BED files with only a few thousand regions to fit within the 8,192-token limit, severely restricting the diversity and complexity of bulk datasets that could be analyzed (@atacformer-bedbase). To address this, one could explore cutting-edge long-context architectures such as sparse attention mechanisms (Longformer @Beltagy2020, BigBird @Zaheer2021), memory-efficient implementations (Flash Attention @Dao2022), or hybrid architectures inspired by ChromFound @Jiao2025 that combine local and global attention patterns. Target context lengths of 50,000–100,000 tokens would enable processing of full bulk ATAC-seq samples, allowing models to capture long-range regulatory interactions and global chromatin architecture. While these approaches may require high-performance computing environments, the ability to integrate bulk data alongside single-cell data would create a unified framework for analyzing chromatin accessibility across experimental modalities. This capability would be particularly valuable for platforms like BEDbase, which curates large collections of bulk genomic datasets.

==== Expected impact
Optimizing context windows in both directions would produce complementary benefits. Smaller, efficient models would democratize access to foundation model capabilities, enabling local deployment, faster iteration during analysis, and broader adoption in resource-limited settings. This would parallel the success of sentence-transformers in making NLP embeddings accessible to any researcher with a laptop. Conversely, extended context models would unlock comprehensive analysis of bulk ATAC-seq data, enabling integration of diverse experimental modalities and providing insights into genome-wide regulatory architecture that are currently inaccessible. Together, these advances would create a flexible ecosystem of models tailored to different computational environments and biological questions—from lightweight tools for rapid exploration to heavyweight models for in-depth analysis of complex regulatory landscapes.
