=== Future aim 1: Scaling the training Atlas
#figure(
  image("/fig/future-directions/expanded_atlas.svg"),
  caption: [Overview of the expanded training atlas and its components.]
) <future-directions-expanded-atlas>

==== Motivation
Atacformer was trained on a substantial corpus of single-cell ATAC-seq data (\~1.2 million cells); however, this training set remains biased toward well-characterized tissues, cell types, and experimental conditions. Specifically, the dataset is currently dominated by brain, immune, and tonsil cells (@atacformer-overview\B). This bias is reflected in model performance, with reduced performance when processing datasets originating from biological contexts outside the models training distribution (kidney, liver, rare cell types; @atacformer-multi-dataset-analysis). In contrast, foundation models in natural language processing (NLP) have demonstrated that massive, diverse training corpora are critical for achieving strong generalization and reducing bias @Brown2020. To achieve similar benefits in regulatory genomics, scaling the training atlas to encompass tens or hundreds of millions of cells spanning all major human tissues, developmental stages, and disease states would be a logical next step. Indeed, foundation models for transcriptomics are already being trained on datasets of this scale with an updated Geneformer model being trained on over 100 million cells @Chen2024b and the release of Tahoe-x1, a new transcriptomics foundation model trained on 100 million cells @Gandhi2025. Expanding the training corpus for chromatin accessibility models like Atacformer could improve generalization and reduce bias in downstream analyses.

==== Proposed approach
Several recent technological developments now make it feasible to scale training corpora to 100 million cells or more. First, leveraging the computational scalability of tools like BPCells @Parks2025, which offer improved memory efficiency for processing large-scale datasets compared to our initial preprocessing pipeline (SnapATAC2 @Zhang2024), would facilitate such expansion. Second, Oxbow @Abdennur2025 — a unification layer built on Apache Arrow with Rust-based I/O—enables efficient handling of next-generation sequencing data as in-memory or larger-than-memory data frames across Python and R, streamlining cross-platform data integration. Together, these advances address the storage, I/O, and processing bottlenecks that previously limited large-scale data curation.

Building on this infrastructure, one could systematically collect single-cell ATAC-seq datasets from public repositories including GEO, SRA, the Human Cell Atlas, and CellXGene, targeting a training corpus exceeding 100 million cells that spans all major human tissues, developmental stages, and disease states. Such an effort could prioritize underrepresented biological systems—diverse neuron subtypes, rare immune populations, and organoid models—to mitigate the tissue and cell-type biases observed in the current Atacformer training set. A robust quality control pipeline would harmonize data from heterogeneous sources, accounting for differences in sequencing depth, peak calling strategies, and experimental protocols. Stratified sampling strategies would ensure balanced representation across tissues and cell types during model training, reducing the risk of overfitting to well-studied biological contexts.

==== Expected impact and evaluation
Scaling the training atlas to 100 million cells would be expected to yield models with substantially improved generalization across diverse biological contexts. Such models could demonstrate better zero-shot performance on previously unseen tissues and cell types, reduced bias in cell-type annotation and clustering, and more robust representations for rare cell populations currently underrepresented in existing training data. These improvements would establish a stronger foundation for clinical applications that require accurate modeling of diverse disease contexts and developmental stages.

To quantitatively assess these improvements, one could evaluate retrained models using the same cell-clustering benchmarks established in the original Atacformer work. Specifically, measuring clustering quality metrics (silhouette scores, adjusted Rand index) across held-out datasets spanning diverse tissues would allow comparison of performance between models trained on the original corpus versus an expanded 100-million-cell atlas. Particular attention could be paid to performance gains on underrepresented tissues (kidney, liver, rare immune populations) where the current model shows reduced accuracy. Additionally, assessing the model's ability to generalize to completely novel datasets not represented in either training corpus would provide a rigorous test of improved generalization. Finally, the curated training corpus could be released as a community resource to support reproducibility and enable other researchers to build upon this foundation.
