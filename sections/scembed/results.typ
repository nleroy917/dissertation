#import "@local/dissertation:0.0.1": figure-caption-extended, todo

=== Overview of the scEmbed architecture 
#figure(
  image("/fig/scembed/overview.svg"),
  caption: [An overview of the scEmbed architecture and training procedure.]
) <scembed-overview>
#figure-caption-extended(caption: [
*a.* scEmbed leverages Word2Vec as its core model. Word2Vec learns to predict words given a semantic context. Similarly, scEmbed learns to predict genomic regions, given a genomic context. This is unsupervised, and uses the patterns of genomic region co-occurrence to learn representations of individual regions. *b.* Overview of the scEmbed learning process, starting with scATAC-seq data.  *c.* Once region embeddings are learned, they can be used to construct cell embeddings by averaging the embeddings of regions accessible in each cell. We use cell embeddings for downstream tasks of clustering and cell-type prediction. *d.* Diagram showing three steps of the benchmarking process
])

To build a novel neural network that can learn dense, low-dimensional vectors of genomic regions, we designed scEmbed. scEmbed adapts our previous work, Region2Vec @Gharavi2021a, to single cells. The model is a modified unsupervised Word2Vec @Mikolov2013a model that learns to predict genomic region co-accessibility similarly to Word2Vec (@scembed-overview \A). scEmbed consists of a single embedding layer followed by a context prediction layer for training. Briefly, scEmbed treats each cell as a document and its accessible regions as words. Context is simulated by repeatedly shuffling these regions (@scembed-overview \B). We experiment with both skip-gram (SG; predicting context regions given a target region) and continuous bag-of-words (CBOW; predicting a target region given its context) training objectives. After training, we are left with a set of region embeddings for each genomic region in the model vocabulary. To construct cell embeddings, we average region embeddings for each cell, which are then used for tasks like clustering, analysis, or transfer learning (@scembed-overview \C).

To validate scEmbed, we followed an earlier approach @Chen2019a to benchmark it on clustering tasks using published reference scATAC data. We experiment with a diverse set of both real and simulated scATAC-seq datasets with known and unknown cell-type labels. These embeddings are clustered using three clustering methods: K-means, hierarchical clustering (HC), and Louvain clustering. The clusters are then subjected to evaluation using established metrics (@scembed-overview \D). Finally, we explore the potential of scEmbed for transfer learning tasks by repurposing existing models.

=== scEmbed is competitive with existing scATAC-seq methods

#figure(
  image("/fig/scembed/benchmarking.svg"),
  caption: [Benchmarking shows that scEmbed is competitive with existing approaches.]
) <scembed-benchmarking>
#figure-caption-extended(caption: [
*a.* UMAP plot of the cell-embeddings produced by scEmbed. *b.* Results of the benchmarking pipeline. scEmbed is competitive with the top methods. We tested three clustering methods: Hierarchical clustering (HC), K means, and Louvain. The clustering results were evaluated using three metrics: Adjusted mutual information (AMI), adjusted rand index (ARI), and homogeneity. *c.* UMAP plots visually showing the resultant clusters of cell embeddings produced by scEmbed following data loss. *d.* Line plots showing the change in three clustering metrics (ARI, AMI, and Homogeneity) as a function of dropout rate. Plots show that scEmbed retains its ability to accurately cluster single cells up to nearly 80% data loss.
])

When benchmarked against _Buenrostro2018_ @Buenrostro2018, scEmbed clusters cells of the same type visually (@scembed-benchmarking \A). scEmbed performed similarly to the best-performing scATAC-seq methods, including SCALE, scBasset, cisTopic and SnapATAC (@scembed-benchmarking \B). This performance was achieved with minimal preprocessing of the data and a completely unsupervised learning workflow. In addition to the _Buenrostro2018_ dataset, we also benchmarked scEmbed on another, more recent and comprehensive scATAC-seq dataset from Luecken et al. @Luecken2021. Again, comparing clusters with ground truth labels, scEmbed performs well (@scembed-luecken2021-umaps).

=== scEmbed is robust to data loss

Next we wondered if we could leverage scEmbed for transfer learning tasks, which can result in a loss of information. As such, we sought to evaluate its ability to cluster data with increasing levels of information loss. To test scEmbed’s robustness to missing data, we trained the model on datasets of increasing sparsity. Starting with the _Buenrostro2018_ dataset (2.8% non-zero) @Buenrostro2018, we randomly dropped non-zero values in the binary accessibility matrix until approximately 80% of the data was lost. A dropout rate of 80% resulted in a matrix that was 0.5% non-zero. Even at a drop-out rate of 80%, scEmbed was able to visually cluster cells of the same type (@scembed-benchmarking \D). To quantify this, we computed three scores for each dropout dataset: 1) Adjusted Rand Index (ARI), 2) Adjusted Mutual Information (AMI), and 3) Homogeneity scores. We found that scEmbed retained clustering accuracy comparable to other scATAC-seq analysis methods @Chen2019a even when faced with 80% data loss (Fig. @scembed-benchmarking \E). These findings confirm that scEmbed can learn rich biological knowledge, even for the most sparse datasets. The ability to handle sparseness is a critical characteristic of scATAC-seq analysis, and particularly so for scEmbed, which can be used to transfer information from existing models, as we describe next.

=== Using scEmbed to transfer knowledge of genomic region co-occurrence to unseen datasets 

#figure(
  image("/fig/scembed/projection.svg"),
  caption: [scEmbed enables knowledge transfer to unseen datasets.]
) <scembed-projection>
#figure-caption-extended(caption: [
  Transfer learning with scEmbed occurs in three steps. *a.* Diagram of overlap analysis depicting how a new cell from a new dataset (blue) is tokenized into the feature space of the data used for the pre-trained model (red). *b* Diagram showing the computation of embeddings for new, unseen data. This is achieved using average pooling of region embeddings. *c.* UMAP plots of both projected (right) and unprojected (left) datasets. The plots show nearly identical clustering of embeddings learned from the original dataset versus projection. *d.* RAGI score plots for both the original dataset embeddings and projected cell embeddings. RAGI scores are computed for three clustering methods: Hierarchical clustering, K-means, and Louvain.
])

A key innovation in scEmbed is that it uses a two-step training process, rather than the common single-step approach. In the first step, scEmbed learns embeddings of genomic regions rather than cells. In the second step, the region embeddings are used to build cell embeddings. An advantage of this two-step approach is that the region embeddings can be used to build cell embeddings for new datasets (METHODS?). More importantly, however, this approach allows scEmbed to leverage pre-trained models from reference datasets (@scembed-projection-methods). We call the process of using a pre-trained model to generate embeddings of new data "projection".

#todo("Do we need to expand the above? The NARGAB formatting makes the published paper not ammenable to copy-paste into this.")

We next sought to assess this projection process by asking whether scEmbed could cluster a new dataset based entirely on a pre-trained model. First, we trained a model on the original Buenrostro2018 dataset @Buenrostro2018; second, we took a new dataset, 10X genomics 5k PBMCs from a healthy donor, and tokenized each cell's regions into the original models vocabulary (@scembed-projection \A, #link(<scembed-tokenization>, "see Methods")), followed by region pooling via average pooling (@scembed-projection \B).

We used these single-cell embeddings directly for UMAP visualization and clustering analysis. To assess the quality of the projection, we assumed that ~8 distinct cell populations existed and took advantage of marker gene analysis to assign labels to each cell. We use the Residual Average Gini Index (RAGI) score to evaluate the clustering ability of scEmbed @Chen2019a (Methods). We found that the projected cell analysis showed no marked differences in clustering proficiency when compared with the embeddings produced by conventional model training. The UMAP plots were visually similar (@scembed-projection \C), indicating similar clustering performance.

To further explore the difference, we next evaluated clustering performance using a repeated subsampling strategy that consisted of four steps: (i) train a new model on the PBMC data alone; (ii) repeatedly subsample 1000 cells and compute their embeddings using the new model and the Buenrostro2018 model using projection; (iii) cluster the cells using three strategies (HC, K-means and Louvain); and (iv) compute the RAGI score with these 1000 subsampled cells. The scores were then averaged across all subsamples. Our results showed that the RAGI score between the original and projected datasets did not differ significantly, indicating similar clustering performance (@scembed-projection \D).

#todo("Replace the figure with the correct version that has the correct RAGI plots.")

=== Pre-trained models from reference datasets can be used to annotate cell clusters 
#figure(
  image("/fig/scembed/annotation.svg"),
  caption: [Pre-trained embedding models can be exploited for cell-type annotation tasks.]
) <scembed-annotation>
#figure-caption-extended(caption: [
  *a.* Diagram showing scEmbed’s three projection paths. *b.* Overview of the standard “no-projection” data flow. *c.* Overview of three data flows for new data. EV-projection places the new data in the same latent space as the reference data. *d.* UMAP plot of the reference data embeddings built using the standard workflow. *e.* UMAP plot of the new PBMC data with E-projection. *f.* Plots showing the EV-projection data flow applied to the new PBMC dataset. Grey cells represent the reference topology; colored cells are projected new PBMC data. Separate plots depict individual clusters for visual clarity. *g.* Confusion matrix of scEmbed classification results compared to Cellcano. *h.* UMAP plots showing the cell labels assigned by Cellcano (left) and the cell type labels assigned by scEmbed (right).
])

Convinced that pre-trained models could be used to visualize an unseen query dataset, we next asked whether this approach could be used to annotate cell types without training a model. We first built a reference model using the Luecken2021 multi-omic dataset @Luecken2021, a first-of-its-kind multimodal benchmark dataset of 120,000 single cells from the human bone marrow of 10 diverse donors measured with two commercially available multimodal technologies. Using scEmbed and the 'no projection' data flow (see #link(<scembed-projection-methods>, "methods")), we trained a model and clustered the resulting embeddings (@scembed-annotation\A). This model served as the reference for all downstream experiments with a new PBMC dataset from 10X genomics. Using E-projection, scEmbed creates visually distinct clusters of single cells (@scembed-annotation\B). To visualize these embeddings in the context of the original embedding topology, we employ EV-projection. Each identified cluster from E-projection aggregates to a distinct location in the original UMAP embedding topology of the Luecken2021 model (@scembed-annotation\C).

Confident our pre-trained Luecken2021 embedding model was distinctly clustering the new PBMC dataset, we sought to assign cell-type labels to each cluster. We used Cellcano, a new scATAC-seq cell annotation method, to assign ground-truth labels to each cluster of the E-projected PBMC embeddings for evaluation of our method @Ma2023 (@scembed-annotation\D). Our cell-type annotation system was limited by the cell types annotated in Luecken2021; as such, we mapped each scEmbed prediction class to a corresponding Cellcano class for comparison (@scembed-cellcano-label-mapping) after following their annotation procedure (see the #link(<scembed-cellcano-methods>, "scEmbed materials and methods")\; @scembed-cellcano-tsne). Using a simple k-nearest-neighbor (KNN) classification algorithm, scEmbed was highly consistent with the Cellcano labels (F1 = 0.87, @scembed-annotation\E). However, without class mapping, scEmbed offers higher specificity of cluster identity and even identifies a cluster of ID2-hi myeloid progenitor cells not found with Cellcano (@scembed-annotation\F). Moreover, this workflow enables researchers to quickly try new models trained on many different cell types and rapidly discover cell types in their data. Using our projection system, researchers can avoid training a new model each time they want to use a new reference dataset, which is a common approach in many modern cell-type annotation systems @Ashuach2022 @Ma2023 @Altay2024. The entire process of dimensionality reduction, clustering and annotation took $<$10 min on a laptop, and we observed similar time savings across several models (FIGURE FOR THIS?). Thus, we conclude that EV-projection is a promising approach for fast visualization and annotation of new data.