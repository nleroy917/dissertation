The exponential growth of publicly available genomic data has spurred the development of powerful pre-trained foundation models across diverse genomic modalities, including DNA sequences (e.g. Enformer @Avsec2021 and DNA Discrete Diffusion @Sarkar2024); gene expression (e.g. Geneformer @Theodoris2023 and scGPT @Cui2024); and protein folding (e.g. AlphaFold @Abramson2024 @Jumper2021 @Senior2020). By leveraging transfer learning, these models have dramatically enhanced data integration capabilities, enabling researchers to apply knowledge learned from large-scale datasets to new, often smaller-scale tasks.

Despite significant progress across genomics, transcriptomics, and proteomics, the development of foundation models explicitly tailored for epigenomic data remains comparatively underexplored. The Assay for Transposase-Accessible Chromatin sequencing (ATAC-seq) and its single-cell variant (scATAC-seq) @Buenrostro2013 @Buenrostro2015 have emerged as key methods for interrogating the regulatory landscape of the genome, offering insights into disease mechanisms, cellular heterogeneity, and tissue development. However, these assays present computational challenges due to their high dimensionality and inherent data sparsity @Sheffield2012 @Smith2020. Consequently, a wide array of computational tools have been developed to manage these complexities and simplify analysis pipelines for researchers. Comprehensive platforms, including ArchR @Granja2021 and SnapATAC @Fang2021a @Zhang2024, provide scalable, end-to-end solutions that support many standard workflows, such as quality control, dimensionality reduction, and trajectory analysis. More advanced, specialized tools like cisTopic provide a probabilistic topic modeling framework for discovering co-accessible enhancers and regulatory patterns, while deep learning approaches like SCALE @Xiong2019 and scBasset @Yuan2022 seek to address fundamental challenges of data sparsity and learning relationships between DNA sequence and chromatin accessibility.

These tools have pushed forward the analysis of scATAC-seq data. However, current methods for scATAC-seq are specialized, task-specific, and limited to the immediate dataset under investigation. They fail to exploit the shared biological knowledge encoded in the many publicly available datasets. No method has focused on producing a foundation model, trained on vast and diverse datasets, designed to be adapted to both new models through downstream fine-tuning and to new datasets through  transfer learning. Recognizing this limitation, new models for scATAC-seq are emerging. This includes ChromFound @Jiao2025 and EpiAgent @Chen2024, which are the first foundation models explicitly tailored for scATAC-seq, utilizing large-scale pre-training on millions of cells. Despite their advances, these models have several limitations. First, they are confined to single-cell scATAC-seq data and do not handle bulk ATAC-seq region-sets, which are widely used in many contexts. Second, they do not model genomic regions as discrete tokens, opting instead for continuous representations of cells that limit interpretability. Third, they rely on very large architectures -- for example, EpiAgent uses over 1 billion parameters across its embedding and transformer modules, which requires a GPU for inference and adoption. Fourth, they do not explore multimodal contrastive integration with scRNA-seq, which restricts cell-type transfer and cross-modal inference capabilities @Schaefer2024.

To address these limitations, we present Atacformer, a transformer-based foundation model. Atacformer leverages large-scale pre-training on single-cell ATAC-seq data. Furthermore, unlike existing approaches, we model genomic intervals as discrete tokens -- the fundamental "words" of the regulatory genome. This token-based representation aligns with biological intuition and allows Atacformer to exploit the strengths of transformer architectures, which are particularly well-suited for learning from sequences of discrete inputs, as demonstrated in Natural Language Processing. Beyond the Atacformer model, we also introduce a dual-encoder contrastive learning approach called Contrastive RNA-ATAC-Fine-Tuning (CRAFT), which supports cross-modal alignment between scATAC-seq and scRNA-seq data. We benchmarked the performance of Atacformer and CRAFT models across four key applications. First, we assessed zero-shot cell clustering on multiple new, unseen PBMC datasets after fine-tuning the model on a cell-type clustering task. Next, we tested direct fragment file processing to measure speed and biological concordance without conventional preprocessing. Third, we applied Atacformer to bulk region-set data to evaluate embedding quality and metadata prediction. Finally, we explored a dual-encoder model for integrating ATAC-seq and RNA-seq modalities. Our results demonstrate that Atacformer and CRAFT can achieve state-of-the-art performance with best-in-class runtime for scATAC-seq analysis while maintain powerful, zero-shot clustering performance on new, unseen datasets.
