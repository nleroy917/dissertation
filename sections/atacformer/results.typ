#import "@local/dissertation:0.0.1": figure-caption-extended, todo

=== Atacformer is a new transformer-based foundation model for ATAC-seq data
#figure(
  image("/fig/atacformer/overview/overview.svg"),
  caption: [An overview of the Atacformer architecture and training procedure.]
) <atacformer-overview>
#figure-caption-extended(caption: [*a.* Model architecture and pretraining schematic for Atacformer. Individual cells are tokenized into the model universe, followed by random token replacement. These tokens are then passed to the embedding module, followed by $n$ transformer blocks to generate contextualized embeddings. *b.* Tissue distribution and representation in the scATAC atlas used in Atacformers pretraining. *c.* The Atacformer tokenization strategy. New cells are tokenized into the model vocabulary using interval overlap analysis.
])

Atacformer is a transformer-based @Vaswani2017 foundation model that produces contextualized embeddings of genomic regions. Unlike existing models that are large, rigid, and task-specific, Atacformer is general-purpose and lightweight: users need only provide chromosome coordinates (chromosome, start, end) to begin analysis. By minimizing assumptions about the data and streamlining inputs, Atacformer serves as a fast, efficient backbone that can be adapted to virtually any genomic interval task. Atacformer consists of two main components: a genomic region embedding module and a stacked transformer encoder layer with multi-head attention (@atacformer-overview\A). To train Atacformer, we curated a single-cell ATAC-seq atlas consisting of 1.2 million cells and $>$ 10 billion tokens from 30 tissues (@atacformer-overview\B; @atacformer-pretraining-dataset-table) @Zhang2021b @Herring2022 @Massoni-Badosa2024 @Lee2023 @Patel2022 @Collin2021. We uniformly processed all raw datasets using a standardized pre-processing pipeline to ensure data integrity and compatibility (see Methods). We used these uniformly processed results to create a unified consensus vocabulary based on our earlier work @Rymuza2024 consisting of 890,704 distinct genomic regions (see Methods).

To tokenize a single cell into a set of dense, low-dimensional embeddings, we first map each accessible region in the cell to a corresponding region in the model's vocabulary through simple interval intersection (@atacformer-overview\C). This process transforms noisy, unstandardized genomic intervals into fixed tokens while preserving the biological significance of co-accessibility patterns. To enable extremely fast, in-memory tokenization that supports modern machine learning workflows, we developed a set of Rust-based tokenizers to be used in conjunction with Atacformer @LeRoy2025.

Atacformer is trained using an ELECTRA-style pre-training objective @Clark2020 (Fig. @atacformer-overview\D) in which Atacformer receives tokenized region sets in which a random subset of tokens are replaced with others sampled from the vocabulary. The model is then tasked with predicting which tokens were replaced. Unless noted, we pre-trained Atacformer using a 45% token replacement rate and a context window of 8,192, as this captures the majority of co-accessible regions in all single-cells in our corpus (@atacformer-context-window-distribution). We refer to this model as atacformer-base. We evaluated atacformer-base to establish a performance baseline, and then fine-tuned it to achieve better performance and enable more flexible downstream analyses.


=== Atacformer can be paired with Geneformer for powerful multiomics analysis
#figure(
  image("/fig/atacformer/craft/craft.svg"),
  caption: [CRAFT is a powerful dual-encoder, multimodal single-cell embedding model.]
) <atacformer-craft>
#figure-caption-extended(caption: [*a.* Schematic of the CRAFT training procedure. (Left) Schematic of a multiomic single-cell dataset showing ATAC and RNA signal jointly profiled in a single cell. (Right) Training step for a single mini-batch of cells. *b.* UMAP visualizations of the single-cell embeddings generated using the ATAC-encoder (left) and the RNA encoder (right). *c.* Schematic of the learned co-embedding space after training. *d.* Heatmap of nearest RNA-embedding neighbors to a single ATAC-embedding cell, organized by cell type. *e.* Schematic of the RNA-decoder training procedure. *f.* PBMC5k dataset clustered using ATAC-embeddings; colored by cell-type. *g.* Heatmaps of PBMC5k dataset, colored by predicted RNA-expression profile for four different marker genes. *h.* Distribution of predicted LYZ expression levels across cell types.
])

We sought to fine-tune Atacformer on a multimodal task. Recent breakthroughs, such as CLIP (@Radford2021a), demonstrate that aligning fundamentally different data types in a shared latent space enables zero-shot transfer and cross-modal retrieval. Inspired by this, we investigated whether Atacformer would benefit from being paired with an encoder of another modality, such as scRNA-seq. To investigate, we developed Contrastive RNA-ATAC Fine-Tuning (CRAFT), a multi-modal model that combines Atacformer with Geneformer, a transcriptome encoder (@Theodoris2023). CRAFT uses contrastive training on multiomic data to create a shared latent space for scATAC-seq and scRNA-seq data. We initialized the model with pre-trained Atacformer and Geneformer models. We then trained CRAFT using a multiomic dataset containing over 106,000 cells profiled with scATAC and scRNA simultaneously (@atacformer-craft\A; see Methods).

UMAP visualizations of ATAC-seq and RNA-seq embeddings were topologically similar, highlighting modality alignment (@atacformer-craft\B). Embeddings maintained biologically meaningful relationships; for example, the distinct groups of monocytes and CD8+ T cells are maintained in both modalities, as is a clear linear trajectory of the stages of red blood cell development from proerythroblast to erythroblast to normoblast . When projected into a single two-dimensional UMAP, modalities separated visually (@atacformer-craft-by-modality); but despite this, nearest neighbors in higher-dimensional spaces preserved biological similarities, allowing accurate cross-modal cell-type alignment (@atacformer-craft\C). To assess biological coherence, we projected ATAC embeddings into the multimodal CRAFT latent space and queried their nearest RNA neighbors. Consistently, the local RNA neighborhoods aligned with the same cell type as the corresponding ATAC embedding. (@atacformer-craft\D).

Next, we assessed whether the aligned ATAC embeddings encoded transcriptomic information by training a small decoder to predict RNA-seq profiles from ATAC embeddings (Methods, @atacformer-craft\E). This should allow us to leverage the joint CRAFT embeddings to predict scRNA-seq outputs from datasets where only scATAC-seq was assayed, or vice versa. We applied this decoder to an unseen scATAC-seq dataset with known labels, labeled with scVI (see methods). UMAP projections of the ATAC embeddings visually separated cells into clusters for T Cell, B Cell, and Monocyte lineages (@atacformer-craft\F), showing that these out-of-sample cells are distinguished by the model. Next, we applied the RNA-seq decoder to predict scRNA-seq profiles. The predicted RNA-seq profiles accurately recapitulated known gene expression differences among these lineages, including cell-type-specific markers. For example, the imputed expression of monocyte marker LYZ was elevated in monocytes; B cell marker MS4A1 was elevated in B cells; T cell marker CD3E was elevated in T cells; cytotoxic cell marker GNLY was elevated in cytotoxic cells(@atacformer-craft\G). Finally, we examined the predicted normalized expressed across cell-types quantitatively. As an example, we show that the monocyte marker LYZ is disproportionately up-regulated in Monocyute-like cells (@atacformer-craft\H). Collectively, these results demonstrate that RNA-seq patterns have been incorporated in the ATAC-seq embeddings of the multi-modal CRAFT model.

In total, these results demonstrate that CRAFT effectively integrates Atacformer embeddings with complementary single-cell models, enabling robust multimodal analysis and cross-modal biological inference. This dual-encoder framework accurately aligns chromatin accessibility and transcriptomic data within a unified latent space, facilitating precise cell-type identification and enhancing the interpretability of single-cell chromatin profiles.

=== Fine-tuned Atacformer models and CRAFT enable fast and accurate zero-shot cell-clustering
#figure(
  image("/fig/atacformer/clustering/clustering.svg"),
  caption: [Atacformer clusters new scATAC data accurately in a zero-shot approach.]
) <atacformer-clustering>
#figure-caption-extended(caption: [*a.* Schematic of the supervised cell-type fine-tuning task using triplet loss. *b.* Runtime comparisons between Atacformer and other popular methods for clustering scATAC-seq data. *c.* Clustering performance of Atacformer-base and Atacformer-ctft on three separate PBMC datasets compared with other popular methods for clustering scATAC-seq data. *d.* UMAP plots highlight the latent space change when fine-tuning Atacformer on cell types. *e.* Runtime versus ARI chart for Atacformer and other popular methods.])

The first bottleneck in scATAC-seq analysis is cell clustering. Most current tools either retrain on every dataset or take minutes per thousand cells. We sought to improve cell-clustering without sacrificing speed or requiring any additional processing. A pre-trained Atacformer model could possibly cluster unseen data very quickly; however, we reasoned that a model trained on an unsupervised token replacement prediction task would perform sub-optimally for a cell-type clustering task. To that end, we sought to improve the single-cell embeddings in two ways: First, inspired by the Sentence-BERT (SBERT) family of text embedding models @Reimers2019, we designed a novel fine-tuning approach for cell-type clustering. Starting with the base model (`atacformer-base`), we trained a *cell-type fine-tuned* version, `atacformer-ctft` using triplet loss to position cells of the same type together in the latent space (@atacformer-clustering\A; see Methods). We used the Luecken2021 multi-omics PBMC dataset for fine-tuning, which provides high-confidence cell-type labels across matched modalities @Luecken2021. Second, we utilized our previously described CRAFT model. Our reasoning was that the resultant single-cell embeddings from CRAFT would be much higher quality through mutual refinement of ATAC-seq and RNA-seq signal -- a phenomenon seen in other contexts @Peng2025 @Liu2024.

To assess both the base and fine-tuned models, we collected three separate datasets for evaluation: 1) A third-party, pre-annotated brain dataset, 2) a simulated PBMC dataset from bulk ATAC-seq data, and 3) a pre-annotated PBMC dataset. We generated cell embeddings for all cells in each dataset, and then used them to benchmark `atacformer-base`, `atacformer-ctft` and CRAFT against Principal Component Analysis (PCA) and several popular methods for scATAC-seq clustering @Xiong2019 @LeRoy2024 @Zhang2024 @Chen2024. To establish Atacformer's capabilities for clustering data it's never seen before (i.e. zero-shot clustering), all datasets were explicitly excluded from the training data for both `atacformer-base`, `atacformer-ctft`, and CRAFT.

We found Atacformer to be one of the fastest methods evaluated; the only faster method being our previous Word2Vec-based method scEmbed @LeRoy2024 (@atacformer-clustering\B). The slowest methods were those that required the use of a very large model (EpiAgent, 1.5B parameters @Chen2024), or required training from scratch on a combined dataset with both the original and new data (SCALE).

To evaluate clustering accuracy, we clustered the embeddings obtained from each method to obtain cluster labels. These labels were then compared to the ground-truth cell-type labels using three metrics: Adjusted Rand Index (ARI), Adjusted Mutual Information (AMI), and Homogeneity score (see Methods). As expected, the atacformer-base model underperformed other methods for PBMC-based datasets. The cell-type fine-tuned model increased clustering performance according to ARI by $~ 15%$ across the PBMC datasets (@atacformer-clustering\C). These gains were also reflected in the training dataset for the fine-tuning procedure (@atacformer-clustering-ctft-before-after). Notably, `atacformer-base` performed exceptionally well on the brain dataset, surpassing the cell-type fine-tuned model. We reasoned that this was because the cell-type fine-tuned atacformer model was fine-tuned only on blood data, hurting its performance. Finally, we found CRAFT to perform very well across the datasets, highlighting the power of contrastive learning for embedding quality improvement. To further explore how the fine-tuning step improved the base model for this task, we compared the embeddings of the two atacformer models visually using UMAP. The clustering score gains are clearly reflected in the UMAPs. For example, the cell-type fine-tuning approach significantly improves Atacformers ability to cluster the NK-cells along with differentiating between CD4+ and CD8+ T cells (@atacformer-clustering\D).

In practice, it is common for individual cells to yield more fragments than the model's maximum context window, resulting in more tokens than can be processed in a single forward pass (@atacformer-context-window-analysis\A). We addressed this with two strategies: (1) truncating after the first C tokens (@atacformer-context-window-analysis\B), and (2) randomly sampling $C$ tokens per cell (@atacformer-context-window-analysis\C). We adopted random sampling, reasoning that it better preserves the underlying biological heterogeneity and avoids systematic positional bias. To quantify the effect of context window truncation, we systematically varied the number of tokens sampled per cell and assessed the impact on clustering performance (@atacformer-context-window-analysis). Remarkably, we observed that Atacformer maintains strong clustering accuracy even with a substantial reduction in context window size (@atacformer-context-window-analysis\D). The performance remains robust until severe truncation, after which clustering metrics rapidly decline (@atacformer-context-window-analysis\E). This result suggests that the model is resilient to moderate input reduction, enabling efficient analysis without significant loss in biological resolution.

We highlight CRAFT's striking performance when considering both speed and clustering ability together. For the PBMC5k dataset, CRAFT not only exhibited the best clustering performance, it also generated those clusters in the smallest amount of time (@atacformer-clustering\E), yielding the highest cells-per-second throughput of all methods. In addition, we performed a preliminary batch-correction analysis across multiple PBMC datasets. Visually, Atacformer embeddings integrated as well as EpiAgent's (@atacformer-batch-correction), despite the latter's much larger parameter count. This suggests that Atacformer's zero-shot capabilities hold promise for rapid, multi-dataset integration without retraining, a key requirement for scalable single-cell analysis.
Finally, we omitted ChromFound @Jiao2025, a newly published scATAC-seq foundation model, from these panels since the code is not open-source and ARI/runtime on these exact PBMC benchmarks aren't reported. In their work, they report ARI=0.48 on a similar PBMC dataset, while maintaining a throughput of 4 cells per second. This would make it the second slowest method benchmarked.

=== Atacformer learns global regulatory structure in bulk region set data
#figure(
  image("/fig/atacformer/bedbase/bedbase.svg"),
  caption: [Atacformer generalizes to bulk regulatory datasets.]
) <atacformer-bedbase>
#figure-caption-extended(caption: [*a.* UMAP projection of $~$ 10,000 BED file embeddings from BED files on BEDbase. Points are coloured by assay, revealing discrete clusters of assay types. *b.* The same UMAP projection colored by cell line shows distinct groupings for common cell types, indicating that Atacformer encodes both experimental and biological context without supervision. *c.* UMAP visualization of a set of contextualized region embeddings from an example BED file, colored by TSS distance. *d.* The same UMAP visualization, instead with each region embedding colored by its annotation from the ENCODE SCREEN project. *e.* Schematic of the global TSS distance analysis. The distance of each contextualized region embedding to the pELS centroid is found. In general, regions with a high TSS distance will have a larger distance. *f.* Distribution of Pearson correlation coefficients when correlating a regions annotated TSS-distance to its distance to the pELS centroid. *g.* Confusion matrix for our cell line classifier, limited to the top ten cell lines in the bulk dataset.])

Single-cell assays are redefining chromatin biology, yet aggregate profiles from bulk ATAC-seq, ChIP-seq, and DNase-seq are still far more common, and will likely continue to be produced in the future due to dramatically lower cost, as they provide useful information for homogeneous samples or when cell-to-cell heterogeneity is not important. In recent years the number of BED files uploaded to the Gene Expression Omnibus have increased by more than 10,000 files per year, most of them from bulk experiments of many varieties @Khoroshevskyi2023 @Xue2023. To help manage and interpret this growing archive, we recently developed BEDbase -- a unified platform for aggregating, analyzing, and serving genomic region data. We therefore asked whether Atacformer, trained on single-cell data, could generalize to the more heterogenous and voluminous profiles posted to GEO, and thereby provide practical utility for BEDbase, such as by imputing missing key metadata or enhancing region annotations.

To assess this, we fine-tuned Atacformer on BED files annotated with hg38 on BEDbase. We downloaded and tokenized over 35,000 BED files and continued pretraining atacformer-base using ELECTRA-style token replacement prediction for 10 epochs, yielding atacformer-bb. Using atacformer-bb, we generated embeddings for BED files from the top 10 cell lines and assays in our bulk data training set. The embeddings clustered by both assay type (@atacformer-bedbase\A)  and cell line (@atacformer-bedbase\B), indicating the model learned general patterns of regulatory biology. As shown in @atacformer-bedbase\A, assays such as ChIP-seq, TF ChIP-seq, DNase-seq, and ATAC-seq formed distinct groups, suggesting that the model captured assay-specific signal features. Meanwhile, the (@atacformer-bedbase\B) shows that the same embeddings also separated cleanly by biological source, with individual clusters corresponding to cell lines such as HEK293, K562, GM12878, and HeLa.

To investigate whether token-level embeddings also reflect learned biology, we annotated each of the model's 890,704 regions with its distance to the closest transcription start site (TSS) and its detailed region classification according to the ENCODE registry of cCREs (see Methods). We reasoned that the contextualized region embeddings produced by the transformer would enable rich information to be encoded in the embeddings. When visualizing an example BED file with UMAP, we found that contextualized embeddings clustered by TSS distance (@atacformer-bedbase\C). To confirm that the model is distinguishing promoters from enhancers, we also colored the contextualized region embeddings by their region annotations (@atacformer-bedbase\D). We found that regions clustered broadly into groups of proximal enhancer-like sequences (pELS) and distal enhancer-like sequences (dELS).

We next sought to interrogate whether this trend held broadly for all BED files in the training dataset. For this, we developed a novel assessment score inspired by our previous work on region embedding evaluation @Zheng2024 called the TSS distribution score. Briefly, the score is computed for each region using three steps. First, we identify the pELS centroid in embedding space. Second, each regions embedding distance to this centroid is computed. Finally, we correlate a each regions TSS-distance to its distance to the pELS centroid. In general, we hypothesized that regions close to the centroid would also have corresponding small TSS distances. Conversely, regions far away from the centroid would have corresponding large TSS distances.

We found that the contextualized embeddings for BED files consistently has TSS distribution scores greater than zero, reinforcing the fact that the contexualized embeddings are capturing learned regulatory biology across cell lines and assay types. As a control, we also computed the TSS distribution score for shuffled labels, which yielded a distribution of scores centered at zero (@atacformer-bedbase\F).

Finally, we sought to use Atacformer to help annotate data on BEDbase. Many files in the database are missing critical metadata -- particularly cell line annotations (@atacformer-bedbase-assay-cell-line-dists). We therefore hypothesized that Atacformer could be used to impute this missing data and built a proof of concept pipeline to annotate missing cell line information for all BED files. Specifically, we hypothesized that the BED-file embeddings could be used as input to a classifier to predict the missing annotation.

To build the pipeline, we split our dataset into two groups: 1) BED-files with annotated cell lines and 2) BED-files with missing cell line annotations, but they can be inferred from the file description (see Methods) (@atacformer-bedbase-cell-line-imputation). We use the BED-files with annotated cell lines to train an XGBoost decision tree model @Chen2016, and then subsequently use the trained model to predict the cell lines for the BED-files with missing annotations, using the inferred annotations as ground truth labels for evaluation. The decision tree model was effective at annotating the missing cell lines. The model achieved an F1 score of 0.85 and an accuracy of 86% across over 275 cell lines (@atacformer-bedbase\G). Taken together, these results highlight how Atacformer is able to generate powerful embedding representations of bulk genomic interval data.

=== Direct raw-fragment processing with atacformer accelerates scATAC analysis
#figure(
  image("/fig/atacformer/fragments/fragments.svg"),
  caption: [Atacformer is the only method that operates on sequence fragments directly.]
) <atacformer-fragments>
#figure-caption-extended(caption: [*a.* Schematic showing the difference between Atacformers ability to generate embeddings for fragments files and other current scATAC-seq foundation models. *b.* Fragment size distribution of tokenized fragments using the Atacformer tokenizers. *c.* Speed comparisons between Atacformer and other scATAC-seq processing pipelines for importing and filtering fragments and generating single-cell embeddings. *d.* UMAP visualizations for single-cell embeddings starting with fragments files for ArchR, Atacformer, and SnapATAC2.])

One of the most common starting file-formats for scATAC-seq analysis is the fragments file from 10X genomics. This is a BED-like file that contains aligned reads for single-cells, organized by barcode. Therefore, it is desirable for scATAC-seq workflows to take fragments files as input for cell clustering. However, existing scATAC-seq deep learning models such as EpiAgent, ChromFound, and SCALE require count matrices as input. This necessitates a pre-processing step using a tool like ArchR or SnapATAC2 @Granja2021 @Zhang2024 to generate the necessary input format when working with fragments files. In contrast, because of its tokenization procedure, Atacformer is uniquely positioned to generate single-cell embeddings immediately from fragments files. By leveraging interval-overlap-based tokenization, Atacformer bypasses the need for tedious preprocessing steps and produces embeddings directly from fragments, enabling more flexible pipelines and zero-shot analyses. This can improve both speed and flexibility in analysis pipelines (@atacformer-fragments\A).

To examine this, we obtained a new 3,000 cell dataset from flash-frozen human healthy brain tissue from 10X genomics. We call this brain3k. Starting from the fragments file, we processed this data in three ways: 1) using Atacformer and its native tokenizers, 2) using SnapATAC2, and 3) using ArchR. We organized the processing into two main steps: 1) fragments import and filtering, and 2) embedding generation. We measured the time it took to conduct each step for each method (see Methods for details). We then used the resultant embeddings to generate UMAP visualizations. Finally, we took the processed count matrices and fed them to Atacformer and EpiAgent generating embeddings and corresponding UMAP visualizations, similarly measuring the time it took to conduct each step.
We first utilized the Atacformer tokenizers to import and tokenize the fragments, filtering by barcodes. This yielded a fragment-size distribution consistent with standard scATAC-seq datasets (@atacformer-fragments\B). In timed comparisons, the end-to-end wall time for Atacformer -- comprising fragments import/filtering and embedding generation -- was substantially lower than either SnapATAC2 or ArchR (@atacformer-fragments\C). Most of the runtime in the baseline pipelines was spent on I/O-heavy import and matrix construction, whereas Atacformer's Rust-backed interval-overlap tokenization minimizes this overhead by operating directly on the fragments file @LeRoy2025.

Using the resulting embeddings, we visualized cells with UMAP and performed Leiden clustering. Atacformer produced coherent, well-separated clusters that were comparable to or better resolved than those obtained from matrices built with SnapATAC2 or ArchR (@atacformer-fragments\D). Notably, Atacformer achieves this in a zero-shot manner -- without peak calling or an intermediate peak-by-cell matrix - by leveraging fragment co-occurrence structure within cells.
Together, these results demonstrate that Atacformer enables practical, faster, and more flexible scATAC-seq workflows: embeddings can be generated directly from fragments, eliminating prerequisite matrix construction and supporting downstream analyses (e.g., clustering) with minimal preprocessing (@atacformer-fragments\A-D).
