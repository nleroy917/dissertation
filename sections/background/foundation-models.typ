=== Foundation models in genomics
One of the most exciting developments in machine learning has been the emergence of large, pre-trained models that can be fine-tuned for specific tasks with relatively little additional data. These models, often referred to as "foundation models," have demonstrated remarkable performance across a wide range of applications, from image recognition to natural language understanding @Devlin2019a @Radford2018 @Radford2021a @Brown2020 @Raffel2023 @Dosovitskiy2021 @Touvron2023 @He2015.

The emergence of foundation models in computer vision (CV) and NLP has inspired similar efforts in the field of genomics. Typically built on the transformer architecture @Vaswani2017, these models are pre-trained on large-scale genomic datasets and can be used in a variety of downstream tasks for zero-shot predictions, few-shot learning, and transfer learning. Examples include DNABERT @Ji2021: bidirectional encoder for DNA sequences, Enformer: a model that predicts gene expression from DNA sequence @Avsec2021, AlphaFold: a model for predicting protein structures from amino acid sequences @Abramson2024 @Jumper2021 @Senior2020, DNA Discrete Diffusion: a model for generating DNA sequences with desired properties @Sarkar2025, scGPT: a transformer-based model for single-cell RNA-seq data @Cui2024, and Geneformer: a transformer-encoder model for scRNA-seq data @Theodoris2023. Overwhelmingly, these models will be pre-trained on large, diverse datasets and then leveraged for transfer learning to apply them to new, unseen datasets and tasks. This makes them particularly attractive as they can be fine-tuned with relatively little data and computational resources compared to training a model from scratch which is often infeasible for many researchers.

=== Foundation models for single-cell ATAC-seq data
While many models exist for DNA sequences, protein sequences, and scRNA-seq data, until very recently, there has been a lack of similar efforts focused on scATAC-seq data. This is likely due to the unique challenges posed by scATAC-seq data, including its high dimensionality, sparsity, and the complex relationship between chromatin accessibility and gene regulation. Moreover, scATAC-seq data is less ubiquitous than other types of genomic data, making it much more difficult to obtain large, diverse training datasets. Beyond the complexity and scarcity of data, however, lies an even more fundamental obstacle: _the lack of a universal 'vocabulary' for chromatin accessibility._

=== A note on tokenization
Regardless of data modality (text, images, DNA sequences), tokenization is a critical first step for all foundation models, as it defines how raw, disparate inputs are mapped into a shared feature space that the model can operate on. In natural language processing, this is achieved through subword vocabularies that segment text into units like words or byte-pair encodings, creating a stable and universal representation of language @Wu2016 @Kudo2018 @Kudo2018a. In computer vision, images are typically partitioned into fixed-size patches that act as tokens, enabling transformers to process them as sequences.

Genomics has followed suit: proteomics and transcriptomics benefit from clear, biologically grounded vocabularies — amino acids for proteins, and nucleotides or k-mers for RNA. Supported by established gene ontologies, this makes it comparatively easy to build well-defined vocabularies and tokens for a model to process. By contrast, single-cell ATAC-seq presents a uniquely difficult tokenization problem: _the natural unit of information in ATAC-seq, a genomic interval, is not standardized across experiments or datasets, and “shared vocabularies” can vary dramatically depending on how peaks are called or which reference regions are chosen_. This lack of a canonical token set has delayed the emergence of foundation models for scATAC-seq, as the success of such models depends on a stable, unified representation of input features across diverse datasets. Despite this significant hurdle, the landscape has begun to change with the recent emergence of foundation models specifically designed to tackle this tokenization challenge.

=== The first scATAC-seq foundation models: EpiAgent and ChromFound
Foundation models for scATAC-seq data have been scarce. However, this landscape has begun to change with the recent emergence of foundation models specifically designed for chromatin accessibility data. Within the last nine months, two notable methods have been introduced: EpiAgent @Chen2025 and ChromFound @Jiao2025. These models represent the first serious attempts to develop large-scale, pre-trained foundation models for scATAC-seq analysis, potentially addressing many of the limitations faced by previous approaches while leveraging the power of modern deep learning architectures. However, these models exhibit fundamental architectural limitations that constrain their flexibility and applicability. ChromFound explicitly relies on continuous accessibility embeddings rather than discrete tokens, sidestepping a crucial architectural innovation of transformers that enables interpretability and computational efficiency. EpiAgent, while using discrete cCRE tokens, employs a TF-IDF ranking scheme that requires access to the entire cell-by-peak matrix to determine token ordering for each individual cell. This matrix-level dependency prevents these models from operating on individual cells in isolation, precluding applications such as bulk ATAC-seq analysis, individual cell-level queries, or efficient vector database lookups for nearest-neighbor classification. These architectural constraints, combined with additional practical limitations, restrict the broader adoption and utility of current scATAC-seq foundation models.

#figure(
  image("/fig/introduction/tokenization.svg"),
  caption: [Overview of tokenization methods for various modalities.]
) <tokenization>

=== Limitations of current scATAC-seq foundation models: EpiAgent and ChromFound
==== Overview of EpiAgent
EpiAgent is a transformer-based foundation model trained on ~5 million single cells or ~35 billion tokens from 31 tissues. The model has about 1.4 billion parameters, divided between an embedding module, an 18-layer bidirectional transformer, and a signal decoder. EpiAgent tokenizes cells by converting each cell’s accessible cCREs into a “cell sentence,” ranking cCREs by TF-IDF values and limiting input length to ~8,192 tokens. EpiAgent uses two novel training tasks for pre-training: cell-cCRE alignment (classifying whether cCREs are accessible given the cell embedding) and signal reconstruction (rebuilding accessibility signals from embeddings). The model outputs contextualized cCRE embeddings and a `[CLS]` cell embedding that capture cellular heterogeneity and regulatory networks. These representations can be adapted for downstream tasks like unsupervised feature extraction, supervised annotation, data imputation, perturbation prediction, batch correction, query mapping, and even in-silico cCRE knockout analysis 

==== Overview of ChromFound
Similarly, ChromFound is a foundation model specifically designed for scATAC-seq data. It was pretrained on 1.97 million cells spanning 30+ tissues and 6 disease conditions, using a massive 1.86 trillion training tokens. The architecture is hybrid, combining a Window Partition Self-Attention (WPSA) module to capture local enhancer–promoter dependencies within ±200 kb of transcription start sites and a Mamba block @Gu2024 for efficient long-range context processing. Its genome-aware tokenization encodes each open chromatin region with three components: chromosome identity, precise genomic coordinates, and continuous (non-binary) accessibility values. ChromFound is pre-trained using a masked reconstruction objective that predicts both zero and non-zero accessibility values, mitigating sparsity and preserving non-binary regulatory information. The model outputs contextualized OCR embeddings and low-dimensional cell embeddings that can be applied in zero-shot or fine-tuned settings for tasks such as cell clustering, batch correction, cell type annotation, cross-omics prediction (inferring gene expression from ATAC), and enhancer–gene link discovery. 

==== Limitations
While both methods have shown promise and broken new ground in the field of scATAC-seq, they still suffer from substantial limitations. First, these methods rely on large, cumbersome models that exceed 1 billion parameters. This scale rivals even some large language models today, making them computationally intractable even with significant compute and GPU resources. Second, these models lack accessibility and reusability. ChromFound, to date, remains closed source, making it impossible to actually use or evaluate. EpiAgent, while open source, lacks a user-friendly system for accessing pre-trained models. The convention in CV and NLP is to share these models through platforms like Hugging Face, which neither EpiAgent nor ChromFound do, making fine-tuning on new datasets near impossible for most researchers.

Third, and critically, these models exhibit fundamental architectural constraints that limit their flexibility. ChromFound uses continuous accessibility values as token embeddings rather than discrete tokens, bypassing the interpretability and computational benefits that discrete tokenization provides in successful foundation models across other domains. While EpiAgent does employ discrete cCRE tokens, its TF-IDF ranking scheme requires the entire cell-by-peak matrix to determine which tokens represent each cell and in what order. This matrix-level dependency means the model cannot process individual cells in isolation. Consequently, these models cannot be applied to bulk ATAC-seq data, cannot perform efficient individual cell queries against vector databases for nearest-neighbor classification, and cannot analyze small numbers of cells independently without the full matrix context. This fundamentally limits their utility for real-world applications where researchers may want to analyze a single new sample, compare individual cells, or integrate data incrementally.

Finally, both models produce dense, high-dimensional cell embeddings that lack interpretability and explainability. While these embeddings may capture biological variation, they do not provide clear insights into which specific regulatory elements or chromatin features drive cellular identity. In contrast, a foundation model that combines truly discrete tokenization with cell-level independence could achieve comparable performance while enabling direct interpretation of individual tokens, supporting flexible deployment across diverse experimental contexts, and dramatically reducing computational complexity.
