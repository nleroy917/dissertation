=== Model architecture and training
We used the `gensim` python implementation of Word2Vec as the core model for scEmbed. Word2Vec has many configurable hyperparameters @Mikolov2013a, including context window size, embedding size, learning rate scheduling and number of epochs. All experiments were conducted with a fixed set of hyperparameters. We used defaults for scEmbed, informed by experiments on Region2Vec optimization @Zheng2024. Specifically, we use a window size of 5 and an embedding dimension of 100. We also use 100 epochs for all experiments unless otherwise noted. We adopt an exponential learning rate schedule with a decay rate of 0.95. After training, we extract the learned region embeddings from the Word2Vec model and subsequently transfer them to an equivalent model in `pytorch`. This improves accessibility and interoperability with other deep learning frameworks.

=== Data and data processing
Detailed overview of datasets:

_Luecken2021_. The Luecken2021 dataset is a multimodal single-cell benchmarking dataset (27). The data is a first-of-its-kind multimodal benchmark dataset of 120,000 single cells from the human bone marrow of 10 diverse donors measured with two commercially-available multi-modal technologies: nuclear GEX with joint ATAC, and cellular GEX with joint ADT profiles. The data was retrieved from the gene expression omnibus (GEO) using the GEO accession GSE194122.

_Buenrostro2018_. The Buenrostro2018 dataset consists of single-cell chromatin accessibility profiles across 10 populations of immunophenotypically defined human hematopoietic cell types (26). Deduplicated single-cell bam files along with a consensus peak set were provided by Chen et. al. (13). Using bedtools (32), region overlaps with the consensus peak set were computed for each bam file at a minimum overlap of 1bp. Using the -c flag, the number of overlaps with each region in the consensus peak set was calculated. Overlap count files were subsequently converted into a cell by peak binary accessibility matrix formatted as a comma-separated-value file (csv). Finally, the binary accessibility csv was converted into a scanpy AnnData object using the scanpy.read csv API. This was used as input to the scEmbed model.

_5k PBMC_. The PBMC dataset comes from 10X genomics and consists of peripheral blood mononuclear cells (PBMCs) from a healthy donor. Three files were downloaded directly from the 10X genomics website: 1) the sparse peak matrix in `.mtx` format, 2) the cell barcode labels in tsv format, and 3) the consensus peak set in bed format. Using Python, along with pandas and scanpy, these files were processed into a scanpy AnnData object. This was used as input to the scEmbed model.

_Synthetic Bone Marrow_. The synthetic bone marrow dataset was described and provided by Chen et. al.(13). The binary accessibility matrix was downloaded directly from the Pinello Lab’s GitHub as a .rds file. Using R, this file was read, parsed, and exported as a csv. Like the previous two datasets, this csv was processed into a scanpy AnnData object using pandas and scanpy.

=== Tokenization of new data <scembed-tokenization>
To tokenize cells, we use our previously designed genomic tokenizers. Briefly, we take an individual cell and identify each region where it shows signal. We define signal as anything greater than zero. These regions are then collected and we use interval overlap arithmetic to create a set of tokenized genomic intervals for each cell. These tokens are then shuffled and fed into the Word2Vec model for training.

=== Model benchmarking and evaluation <scembed-benchmarking-methods>
To validate scEmbed, we followed an earlier approach @Chen2019a to benchmark it on clustering tasks using published reference scATAC data. We leveraged four main datasets in this work: 1) the Buenrostro2018 dataset, single-cell chromatin accessibility profiles from 10 human hematopoietic cell types @Buenrostro2018; 2) Luecken2021, a multimodal single-cell benchmarking dataset of 120,000 single cells from the human bone marrow of 10 diverse donors measured with two commercially available multimodal technologies @Luecken2021; 3) 10X genomics 5k PBMCs, a single-cell dataset of 5000 peripheral blood mononuclear cells from a healthy donor; and 4) a synthetic bone marrow dataset, a binary accessibility dataset described and provided by Chen et al. @Chen2019a.

We benchmarked scEmbed on the Buenrostro2018 dataset @Buenrostro2018 as well as a more recent and comprehensive scATAC-seq dataset from Luecken2021 @Luecken2021. We trained scEmbed for 100 epochs (TODO) then used the resulting region embeddings to construct cell embeddings. Following previous benchmarking procedures, we clustered the cell embeddings with three clustering methods: K-means, hierarchical clustering (HC), and Louvain clustering. There are two scenarios for which we can evaluate clustering: known ground-truth labels and unknown ground-truth labels. The synthetic bone marrow, Buenrostro2018, and Luecken2021 datasets have known ground-truth labels while the PBMC data have unknown ground-truth labels. When ground-truth labels are known, we employ three scores: the adjusted rand index (ARI), the adjusted mutual info score (AMI), and the homogeneity score. When ground-truth labels are not known, we use the Residual Average Gini Index (RAGI) @Chen2019a

=== Dropout experiments
To explore its transfer learning ability and test robustness to missing data, following Xiong et al. @Xiong2019, we evaluated scEmbed on datasets with increasing levels of information loss. Starting from the already sparse Buenrostro2018 cell-feature matrix (2.8% non-zero) @Rymuza2024, we randomly dropped non-zero values in the binary accessibility matrix until ∼80% of the non-zero data were lost, resulting in a matrix that was 0.5% non-zero

=== Residual Average Gini Index
When ground truth labels are unknown, all aforementioned evaluation metrics are no longer applicable. As such, we need a measure that can still evaluate dataset clustering based on what one would expect, given some sort of prior knowledge about the system. For this, we employ a similar strategy described by Chen et. al. called the Residual Average Gini Index (RAGI). Briefly, the RAGI score compares the accessibility of housekeeping genes with previously characterized marker genes @Pliner2019. RAGI measures the average residual specificity of a clustering solution with respect to marker genes, suggesting that a good clustering solution should have clusters enriched for different marker genes and these genes should be highly accessible in only a few clusters, compared to the less informative housekeeping genes.

=== Transfer learning and projection
scEmbed was designed from the outset to facilitate fast, powerful transfer learning. This transfer approach allows scEmbed to take advantage of pre-trained reference models. We call this 'projection' because we 'project' new data into the latent space of the original dataset, creating cell embeddings for new data using a pre-trained model. Projection occurs in three steps: first, we train a model on reference data to produce region embeddings for each region in the reference consensus region set. For datasets where consensus peaks do not exist, methods that create such sets from raw data could be used as a pre-processing step @Rymuza2024. Second, we take a new single-cell dataset and map the regions to the reference consensus region set using region overlaps (FIGURE? METHODS?). This represents each single cell in the new dataset using the set of regions from the reference dataset, for which we also have region embeddings from the reference model. Finally, we compute the average of all region embeddings for each cell in the new dataset (FIGURE?). This approach leverages the information from a larger atlas of accessibility data to analyze a new dataset. In fact, the original training data need not come from scATAC-seq at all. Using this approach, a model trained with bulk ATAC-seq could be used to project scATAC-seq data. This provides an enormous advantage by utilizing the patterns of region co-occurrence from the vast volume of publicly available region set data to inform cell embeddings of single-cell data.

=== Projection visualization and cell-type annotation
==== Projection data flows
We distinguish between three data flows that can occur with scEmbed: no projection E-projection and EV-projection (@scembed-projection-methods \A). First, we train a reference model using the typical no projection flow (@scembed-projection-methods \B). This is the standard workflow of training a new scEmbed model on some input data and visualizing the resulting embeddings by fitting a UMAP model to reduce the dimensionality to two. Then, given a new query dataset, we could analyze it with any of the three data flows (@scembed-projection-methods \C). In a no projection flow, we would not use the reference model at all; we train and visualize using only the new dataset. In embedding-only projection data flow (E-projection), new data are first embedded using the pre-trained reference model.

These embeddings may then be visualized by fitting a UMAP model to reduce dimensionality to two dimensions. The third data flow, and the novel innovation that accomplishes our goal of reference-based visualization, is the embedding and visualization workflow (EV-projection). In this data flow, new data are first embedded using the pre-trained reference model, as in E-projection; then, these embeddings are further projected
through a UMAP model that was fit on the reference data embeddings, rather than newly fit. With the EV-projection flow, plotting the two-dimensional cell representations on top of the reference data UMAP plot allows one to visualize where in the original embedding space the new data ended up. This is possible because the EV-projection re-uses the same topology from the UMAP model fit to the reference data (@scembed-projection-methods \C).

==== Evaluation of cell type annotation
We use Cellcano, a novel scATAC-seq cell annotation method, to assign ground truth labels to our new PBMC data @Ma2023 (@scembed-cellcano-tsne). We follow the online tutorials (https://marvinquiet.github.io/Cellcano/) and leverage their provided reference dataset to process the new PBMC data. Once ground truth labels have been assigned by Cellcano and putative cell types are assigned, we can evaluate the performance of our model using the previously described classification metrics (#link(<classification-methods>, "see common methods")).