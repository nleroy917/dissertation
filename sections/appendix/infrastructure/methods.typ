=== Gtars and uniwig <infrastructure-uniwig-methods>
The uniwig functionality is implemented in Rust as part of our gtars library #link("https://github.com/databio/gtars")[gtars], providing efficient genomic data processing capabilities. The tool accepts BED format files as input and uses efficient multi-threading to compute genome-wide coverage at single-base-pair resolution. Coverage computation is parallelized across chromosomes to maximize performance on multi-core systems. The output consists of three BigWig files representing start, core, and end coverage signals, which are generated using the `bigtools` library for compatibility with existing genomic browsers and analysis tools. A command-line API is provided for integration into existing workflows, with configurable parameters for coverage thresholds and output formats.

=== Universe construction methods <infrastructure-universe-methods>
We implemented several methods for constructing consensus genomic interval sets, or "universes," as part of our `geniml` library #link("https://github.com/databio/geniml")[geniml].

==== Coverage cutoff (CC) universe
This method constructs a universe by identifying a statistically principled coverage threshold. The core idea is to model the probability of any given genomic position being "covered" versus "not covered" (background) across the entire collection of input files. The method then derives a cutoff value by finding all positions where the probability of being covered is greater than the probability of being background.

Practically, this results in a simple rule: a genomic position, $i$, is included in the universe if its coverage frequency, $"freq"_c(i)$, is greater than or equal to the _average_ coverage across the genome $(S_c / g)$, where $S_c$ is the total coverage and $g$ is the genome length. This provides a data-driven approach to defining a consensus set based on signal enrichment. And $S_c$ is computed as:

$
S_c = sum_"i=1"^"g" "freq"_c(i)
$

Practically, we utilize the `pyBigWig` library to import and process the BigWig coverage files generated by `uniwig`. The final universe is constructed using `numpy` for efficient numerical operations.

==== Maximum likelihood (LH) universe
The Maximum Likelihood (LH) universe improves upon the simple coverage model by incorporating information about region boundaries to better preserve distinct genomic features and prevent them from being merged.

This method uses three separate signal tracks as input: starts, cores (coverage), and ends. It then builds a probabilistic model to calculate the likelihood of each genomic position belonging to one of four states: start, core, end, or background. Using dynamic programming, the algorithm finds the most likely sequence of these states across the entire genome, effectively segmenting it into a set of flexible regions. Finally, a filtering step is applied to remove very small or low-likelihood regions to enhance the quality of the final universe.

Coverage data is processed using `pyBigWig` for efficient BigWig file reading, while `numpy` provides the numerical computing foundation for matrix operations and array manipulations in the dynamic programming algorithm. For performance optimization, the method conditionally uses `numba` when available to just-in-time compile the core dynamic programming routine, significantly accelerating the likelihood calculations.


==== Hidden Markov Model (HMM) universe
The Hidden Markov Model (HMM) approach offers a more sophisticated and tunable alternative to the LH method. It models the genome as a sequence of four hidden states: start (0), core (1), end (2), and background (3). The three signal tracks (starts, cores, and ends) are treated as emissions observed from these hidden states.

The implementation utilizes a Poisson emission model with predefined transition matrices and lambda parameters that were empirically optimized. To handle large chromosomes efficiently, the algorithm employs a segmented prediction strategy that identifies regions containing non-zero coverage and applies HMM decoding only to those segments, with background state assigned to empty regions.

The method processes each chromosome independently, reading BigWig files using the `pyBigWig` library with optimized numpy array operations when available. Coverage data is stored as 16-bit unsigned integers to minimize memory usage while maintaining precision. The core HMM computation uses `scipy.stats` for negative binomial quantile calculations and a custom Poisson model implementation for state prediction.

=== Tokenization methods <infrastructure-tokenization-methods>
We developed the `gtars-tokenizers` library as a high-performance tool for mapping genomic interval data to predefined vocabularies, facilitating their use in machine learning applications. Written in Rust, we implement and make available two interval overlap algorithms: 1) a binary interval tree search @Layer2013, and 2) an augmented-interval list method @Feng2019.

=== Tokenization benchmarking <infrastructure-tokenization-benchmarking>
To evaluate the performance of our tokenization methods, we developed a comprehensive benchmarking framework using Python. The benchmarking system utilizes `subprocess` for command execution and `threading` for concurrent memory monitoring during benchmark runs. Memory usage is tracked using the `psutil` library, which provides cross-platform process monitoring capabilities at configurable intervals. Configuration management is handled through `yaml` for flexible test parameter specification, while `polars` provides efficient data processing and CSV export functionality for results analysis. The framework executes multiple repetitions of each benchmark to ensure statistical reliability, measuring both execution time and peak memory consumption. This automated approach enables systematic comparison of different tokenization algorithms across various dataset sizes and configurations.

=== Environment bindings <infrastructure-environment-bindings>
For python binings, we leverage the `pyo3` crate to implement an interface that enables in-memory processing from python. For R-bindings, we use the `extendr` crate to provide a similar interface. The command-line interface is built using the `clap` crate, allowing users to specify input files, vocabularies, and output formats. Finally, we leverage `wasm-bindgen` to compile the library to WebAssembly, enabling its use in web applications and other environments that support WASM.

=== Software and data availability <infrastructure-software-data-availability>
The `gtars`, `uniwig` and the `gtars-tokenizers` libraries are open-source and available on GitHub at #link("https://github.com/databio/gtars"). The universe creation methods are implemented in the `geniml` library, which is also open-source and available at #link("https://github.com/databio/geniml").
